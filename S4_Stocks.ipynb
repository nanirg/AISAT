{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S4 - Stocks \n",
    "\n",
    "El challenge de esta semana trata sobre predicción del precio de stocks basandonos en el conjunto de datos de **NYSE**, que podéis encontrar aquí:\n",
    "\n",
    "[NYSE Dataset](https://www.kaggle.com/dgawlik/nyse)\n",
    "\n",
    "\n",
    "Usaremos el archivo csv _prices_ , que contiene el histórico de los valores tal y como son. La descripción de los valores de cada columna los tenéis en la propia página de kaggle :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "ofsMqGghsmLB",
    "outputId": "a7df272b-5977-46c0-aff8-97c08b3826a4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# os hemos dejado ya las librerias ;)\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from subprocess import check_output\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import  train_test_split\n",
    "import time #helper libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import newaxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga del archivo de datos y vistacillo (head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "osJcrc4-s7qV",
    "outputId": "d2064dbb-93a4-4da2-80a8-0dc6fd645cd6",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-05 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-06 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-07 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-08 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-11 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851259</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZBH</td>\n",
       "      <td>103.309998</td>\n",
       "      <td>103.199997</td>\n",
       "      <td>102.849998</td>\n",
       "      <td>103.930000</td>\n",
       "      <td>973800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851260</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZION</td>\n",
       "      <td>43.070000</td>\n",
       "      <td>43.040001</td>\n",
       "      <td>42.689999</td>\n",
       "      <td>43.310001</td>\n",
       "      <td>1938100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851261</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>53.639999</td>\n",
       "      <td>53.529999</td>\n",
       "      <td>53.270000</td>\n",
       "      <td>53.740002</td>\n",
       "      <td>1701200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851262</th>\n",
       "      <td>2016-12-30 00:00:00</td>\n",
       "      <td>AIV</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>45.450001</td>\n",
       "      <td>44.410000</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>1380900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851263</th>\n",
       "      <td>2016-12-30 00:00:00</td>\n",
       "      <td>FTV</td>\n",
       "      <td>54.200001</td>\n",
       "      <td>53.630001</td>\n",
       "      <td>53.389999</td>\n",
       "      <td>54.480000</td>\n",
       "      <td>705100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>851264 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date symbol  ...        high     volume\n",
       "0       2016-01-05 00:00:00   WLTW  ...  126.250000  2163600.0\n",
       "1       2016-01-06 00:00:00   WLTW  ...  125.540001  2386400.0\n",
       "2       2016-01-07 00:00:00   WLTW  ...  119.739998  2489500.0\n",
       "3       2016-01-08 00:00:00   WLTW  ...  117.440002  2006300.0\n",
       "4       2016-01-11 00:00:00   WLTW  ...  117.330002  1408600.0\n",
       "...                     ...    ...  ...         ...        ...\n",
       "851259           2016-12-30    ZBH  ...  103.930000   973800.0\n",
       "851260           2016-12-30   ZION  ...   43.310001  1938100.0\n",
       "851261           2016-12-30    ZTS  ...   53.740002  1701200.0\n",
       "851262  2016-12-30 00:00:00    AIV  ...   45.590000  1380900.0\n",
       "851263  2016-12-30 00:00:00    FTV  ...   54.480000   705100.0\n",
       "\n",
       "[851264 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga del archivo\n",
    "prices_dataset =  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elegir stock:\n",
    "\n",
    "De todos los stocks que tenéis disponibles, elegid el símbolo que queráis. El símbolo es el código de letras que identifica a las distintas compañías en los distintos índices. En el caso que nos ocupa, los datos se han sacado de NASDAQ.\n",
    "\n",
    "[¿NASDAQ? ¿Wut?](https://en.wikipedia.org/wiki/Nasdaq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MZPr7Np8tdm6",
    "outputId": "2a8a345e-22aa-44ba-dfd3-5bb18495f3cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1762, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Elegir stock, yo usé yahoo\n",
    "yahoo = prices_dataset[prices_dataset['symbol']==]\n",
    "\n",
    "# Coger solo los valores, poned bien el tipo\n",
    "stock_prices = \n",
    "# Dadle la forma adecuada \n",
    "stock_prices = \n",
    "# Comprueba que ha ido bien ;)\n",
    "stock_prices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pintar gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "iDjSH6_evFgJ",
    "outputId": "32e8078d-0031-4e2e-9298-7b6ff32268dc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# podeis utilizar matplotlib os si os hace más usar seaborn sois totalmente libres :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalado\n",
    "\n",
    "Tened en cuenta que para un funcionamiento óptimo los valores a predecir y predictores deben estar en un rango adecuado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "stlrlQUxvLnj"
   },
   "outputs": [],
   "source": [
    "# Podéis hacer un escalado a vuestra manera, si no os apetece explorar mucho, sklearn os ofrece varios por defecto\n",
    "scaler = \n",
    "stock_prices = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/test split\n",
    "\n",
    "Ojito cuidado. Recordar que el train/test split por defecto de sklearn, por bueno que sea, toma muestras de una manera aleatoria, así que el separado de datos en este caso debe ser en un punto concreto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DOmDtHVrvQjy",
    "outputId": "0b5b94d0-2862-4d5c-8c5c-9527fff3a867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1409 353\n"
     ]
    }
   ],
   "source": [
    "# Completar tamaños y partir\n",
    "train_size = \n",
    "test_size = \n",
    "train, test = \n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear el conjunto de datos\n",
    "\n",
    "Os dejamos esta función base para que veáis cómo se puede preparar el conjunto de datos para introducirlo en el modelo. El parámetro `look_back` os permite definir cuantos dias hacia atrás va a poder ver la red para efectuar su predicción. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DHJ1-YUvmtu"
   },
   "outputs": [],
   "source": [
    "# Si funciona, ¿para qué tocas? ;) \n",
    "\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí tenéis libertad para definir esa vista hacia atrás, esto es elección de cada uno. Haciendo distintas pruebas de entrenamiento podréis configurar el valor que veáis que produce mejores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ankJkgDczSsB"
   },
   "outputs": [],
   "source": [
    "# Decidir el valor de look_back\n",
    "look_back = \n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input para la LSTM\n",
    "\n",
    "Las LSTM requieren un input en 3 dimensiones, asi que es necesario recolocarlo y añadir dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LvmM7Dy5z3VT"
   },
   "outputs": [],
   "source": [
    "# y ya 'tá\n",
    "\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del modelo\n",
    "\n",
    "Tened en cuenta los tamaños de entrada y salida. Además, como función pérdida os recomendamos ``mse``, pero podéis elegir otras alternativas:\n",
    "\n",
    "[Métricas keras](https://keras.io/losses/)\n",
    "\n",
    "\n",
    "Os recomendamos una sola capa de LSTM, no debería ser necesario más, pero en caso de que si queráis añadir más, leed bien la documentación de Keras para saber qué parámetros de respuesta de la capa LSTM tenéis que modificar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "xbrev-Co0BIN",
    "outputId": "1000d978-2300-4120-b55e-a41303f412a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=True, input_shape=(None, 4), units=50)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "compilation time :  0.022571325302124023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1)`\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Rellenar\n",
    "\n",
    "start = time.time()\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "print ('compilation time : ', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ATY7reYk0iqo",
    "outputId": "769113fc-d783-4beb-be8b-c1c13a5ec0a8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 1333 samples, validate on 71 samples\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1333/1333 [==============================] - 10s 8ms/step - loss: 0.1538 - val_loss: 0.3778\n",
      "Epoch 2/300\n",
      "1333/1333 [==============================] - 0s 64us/step - loss: 0.0974 - val_loss: 0.2026\n",
      "Epoch 3/300\n",
      "1333/1333 [==============================] - 0s 64us/step - loss: 0.0411 - val_loss: 0.0399\n",
      "Epoch 4/300\n",
      "1333/1333 [==============================] - 0s 87us/step - loss: 0.0172 - val_loss: 5.7997e-04\n",
      "Epoch 5/300\n",
      "1333/1333 [==============================] - 0s 66us/step - loss: 0.0143 - val_loss: 0.0039\n",
      "Epoch 6/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 0.0084 - val_loss: 0.0072\n",
      "Epoch 7/300\n",
      "1333/1333 [==============================] - 0s 65us/step - loss: 0.0058 - val_loss: 8.8787e-04\n",
      "Epoch 8/300\n",
      "1333/1333 [==============================] - 0s 67us/step - loss: 0.0033 - val_loss: 5.5534e-04\n",
      "Epoch 9/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 0.0027 - val_loss: 6.9548e-04\n",
      "Epoch 10/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 0.0022 - val_loss: 6.0648e-04\n",
      "Epoch 11/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 0.0022 - val_loss: 5.1705e-04\n",
      "Epoch 12/300\n",
      "1333/1333 [==============================] - 0s 57us/step - loss: 0.0021 - val_loss: 5.4767e-04\n",
      "Epoch 13/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 0.0019 - val_loss: 6.8447e-04\n",
      "Epoch 14/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 0.0021 - val_loss: 6.1652e-04\n",
      "Epoch 15/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 0.0019 - val_loss: 5.0203e-04\n",
      "Epoch 16/300\n",
      "1333/1333 [==============================] - 0s 64us/step - loss: 0.0019 - val_loss: 5.0543e-04\n",
      "Epoch 17/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 0.0018 - val_loss: 4.9918e-04\n",
      "Epoch 18/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 0.0019 - val_loss: 4.9572e-04\n",
      "Epoch 19/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 0.0017 - val_loss: 4.9015e-04\n",
      "Epoch 20/300\n",
      "1333/1333 [==============================] - 0s 73us/step - loss: 0.0019 - val_loss: 5.2261e-04\n",
      "Epoch 21/300\n",
      "1333/1333 [==============================] - 0s 68us/step - loss: 0.0018 - val_loss: 5.8977e-04\n",
      "Epoch 22/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 0.0018 - val_loss: 5.5833e-04\n",
      "Epoch 23/300\n",
      "1333/1333 [==============================] - 0s 72us/step - loss: 0.0018 - val_loss: 4.7886e-04\n",
      "Epoch 24/300\n",
      "1333/1333 [==============================] - 0s 57us/step - loss: 0.0019 - val_loss: 4.7517e-04\n",
      "Epoch 25/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 0.0018 - val_loss: 4.6900e-04\n",
      "Epoch 26/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 0.0017 - val_loss: 5.3718e-04\n",
      "Epoch 27/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 0.0019 - val_loss: 4.7689e-04\n",
      "Epoch 28/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 0.0017 - val_loss: 4.6232e-04\n",
      "Epoch 29/300\n",
      "1333/1333 [==============================] - 0s 69us/step - loss: 0.0016 - val_loss: 4.8595e-04\n",
      "Epoch 30/300\n",
      "1333/1333 [==============================] - 0s 89us/step - loss: 0.0017 - val_loss: 4.7668e-04\n",
      "Epoch 31/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 0.0016 - val_loss: 6.4835e-04\n",
      "Epoch 32/300\n",
      "1333/1333 [==============================] - 0s 66us/step - loss: 0.0017 - val_loss: 7.4801e-04\n",
      "Epoch 33/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 0.0016 - val_loss: 5.6442e-04\n",
      "Epoch 34/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 0.0014 - val_loss: 4.4852e-04\n",
      "Epoch 35/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 0.0014 - val_loss: 4.4617e-04\n",
      "Epoch 36/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 0.0015 - val_loss: 4.7016e-04\n",
      "Epoch 37/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 0.0016 - val_loss: 4.5165e-04\n",
      "Epoch 38/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 0.0015 - val_loss: 4.8496e-04\n",
      "Epoch 39/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 0.0015 - val_loss: 4.7532e-04\n",
      "Epoch 40/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 0.0015 - val_loss: 4.4810e-04\n",
      "Epoch 41/300\n",
      "1333/1333 [==============================] - 0s 65us/step - loss: 0.0014 - val_loss: 4.6963e-04\n",
      "Epoch 42/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 0.0014 - val_loss: 4.5040e-04\n",
      "Epoch 43/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 0.0015 - val_loss: 4.8550e-04\n",
      "Epoch 44/300\n",
      "1333/1333 [==============================] - 0s 68us/step - loss: 0.0014 - val_loss: 4.3886e-04\n",
      "Epoch 45/300\n",
      "1333/1333 [==============================] - 0s 73us/step - loss: 0.0014 - val_loss: 4.6591e-04\n",
      "Epoch 46/300\n",
      "1333/1333 [==============================] - 0s 77us/step - loss: 0.0014 - val_loss: 4.3921e-04\n",
      "Epoch 47/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 0.0014 - val_loss: 5.3701e-04\n",
      "Epoch 48/300\n",
      "1333/1333 [==============================] - 0s 76us/step - loss: 0.0013 - val_loss: 4.6024e-04\n",
      "Epoch 49/300\n",
      "1333/1333 [==============================] - 0s 94us/step - loss: 0.0014 - val_loss: 7.0698e-04\n",
      "Epoch 50/300\n",
      "1333/1333 [==============================] - 0s 95us/step - loss: 0.0013 - val_loss: 4.9955e-04\n",
      "Epoch 51/300\n",
      "1333/1333 [==============================] - 0s 108us/step - loss: 0.0013 - val_loss: 4.7434e-04\n",
      "Epoch 52/300\n",
      "1333/1333 [==============================] - 0s 94us/step - loss: 0.0013 - val_loss: 5.7312e-04\n",
      "Epoch 53/300\n",
      "1333/1333 [==============================] - 0s 105us/step - loss: 0.0013 - val_loss: 4.3252e-04\n",
      "Epoch 54/300\n",
      "1333/1333 [==============================] - 0s 97us/step - loss: 0.0012 - val_loss: 4.9392e-04\n",
      "Epoch 55/300\n",
      "1333/1333 [==============================] - 0s 94us/step - loss: 0.0012 - val_loss: 4.4540e-04\n",
      "Epoch 56/300\n",
      "1333/1333 [==============================] - 0s 77us/step - loss: 0.0013 - val_loss: 5.1866e-04\n",
      "Epoch 57/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 0.0013 - val_loss: 4.6750e-04\n",
      "Epoch 58/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 0.0012 - val_loss: 5.0087e-04\n",
      "Epoch 59/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 0.0012 - val_loss: 4.8922e-04\n",
      "Epoch 60/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 0.0011 - val_loss: 4.4641e-04\n",
      "Epoch 61/300\n",
      "1333/1333 [==============================] - 0s 71us/step - loss: 0.0011 - val_loss: 4.3099e-04\n",
      "Epoch 62/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 0.0013 - val_loss: 7.7554e-04\n",
      "Epoch 63/300\n",
      "1333/1333 [==============================] - 0s 64us/step - loss: 0.0012 - val_loss: 4.3332e-04\n",
      "Epoch 64/300\n",
      "1333/1333 [==============================] - 0s 65us/step - loss: 0.0012 - val_loss: 4.7153e-04\n",
      "Epoch 65/300\n",
      "1333/1333 [==============================] - 0s 67us/step - loss: 0.0012 - val_loss: 4.3926e-04\n",
      "Epoch 66/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 0.0012 - val_loss: 4.3210e-04\n",
      "Epoch 67/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 0.0013 - val_loss: 5.1180e-04\n",
      "Epoch 68/300\n",
      "1333/1333 [==============================] - 0s 67us/step - loss: 0.0012 - val_loss: 4.3389e-04\n",
      "Epoch 69/300\n",
      "1333/1333 [==============================] - 0s 85us/step - loss: 0.0012 - val_loss: 4.8371e-04\n",
      "Epoch 70/300\n",
      "1333/1333 [==============================] - 0s 66us/step - loss: 0.0012 - val_loss: 4.7741e-04\n",
      "Epoch 71/300\n",
      "1333/1333 [==============================] - 0s 56us/step - loss: 0.0011 - val_loss: 6.0055e-04\n",
      "Epoch 72/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 0.0011 - val_loss: 4.2436e-04\n",
      "Epoch 73/300\n",
      "1333/1333 [==============================] - 0s 76us/step - loss: 0.0011 - val_loss: 5.1976e-04\n",
      "Epoch 74/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 0.0012 - val_loss: 4.2612e-04\n",
      "Epoch 75/300\n",
      "1333/1333 [==============================] - 0s 67us/step - loss: 0.0011 - val_loss: 4.8369e-04\n",
      "Epoch 76/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 0.0011 - val_loss: 4.2612e-04\n",
      "Epoch 77/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 0.0012 - val_loss: 4.2480e-04\n",
      "Epoch 78/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 0.0012 - val_loss: 8.0270e-04\n",
      "Epoch 79/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 0.0010 - val_loss: 4.8171e-04\n",
      "Epoch 80/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 0.0013 - val_loss: 4.2499e-04\n",
      "Epoch 81/300\n",
      "1333/1333 [==============================] - 0s 81us/step - loss: 0.0011 - val_loss: 4.3228e-04\n",
      "Epoch 82/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 0.0011 - val_loss: 4.2998e-04\n",
      "Epoch 83/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 0.0012 - val_loss: 4.5969e-04\n",
      "Epoch 84/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 0.0011 - val_loss: 5.7377e-04\n",
      "Epoch 85/300\n",
      "1333/1333 [==============================] - 0s 65us/step - loss: 0.0011 - val_loss: 4.1825e-04\n",
      "Epoch 86/300\n",
      "1333/1333 [==============================] - 0s 85us/step - loss: 0.0012 - val_loss: 7.6746e-04\n",
      "Epoch 87/300\n",
      "1333/1333 [==============================] - 0s 75us/step - loss: 0.0012 - val_loss: 4.6709e-04\n",
      "Epoch 88/300\n",
      "1333/1333 [==============================] - 0s 66us/step - loss: 0.0012 - val_loss: 4.1748e-04\n",
      "Epoch 89/300\n",
      "1333/1333 [==============================] - 0s 68us/step - loss: 0.0011 - val_loss: 4.8463e-04\n",
      "Epoch 90/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 0.0012 - val_loss: 4.3539e-04\n",
      "Epoch 91/300\n",
      "1333/1333 [==============================] - 0s 72us/step - loss: 0.0011 - val_loss: 5.0420e-04\n",
      "Epoch 92/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 0.0011 - val_loss: 8.2249e-04\n",
      "Epoch 93/300\n",
      "1333/1333 [==============================] - 0s 65us/step - loss: 0.0011 - val_loss: 4.3230e-04\n",
      "Epoch 94/300\n",
      "1333/1333 [==============================] - 0s 65us/step - loss: 0.0011 - val_loss: 4.1903e-04\n",
      "Epoch 95/300\n",
      "1333/1333 [==============================] - 0s 64us/step - loss: 0.0012 - val_loss: 4.6860e-04\n",
      "Epoch 96/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 0.0011 - val_loss: 4.3898e-04\n",
      "Epoch 97/300\n",
      "1333/1333 [==============================] - 0s 75us/step - loss: 0.0011 - val_loss: 4.4312e-04\n",
      "Epoch 98/300\n",
      "1333/1333 [==============================] - 0s 67us/step - loss: 0.0010 - val_loss: 5.5139e-04\n",
      "Epoch 99/300\n",
      "1333/1333 [==============================] - 0s 55us/step - loss: 9.9433e-04 - val_loss: 4.5862e-04\n",
      "Epoch 100/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 0.0011 - val_loss: 5.5445e-04\n",
      "Epoch 101/300\n",
      "1333/1333 [==============================] - 0s 68us/step - loss: 9.8237e-04 - val_loss: 4.3674e-04\n",
      "Epoch 102/300\n",
      "1333/1333 [==============================] - 0s 71us/step - loss: 0.0010 - val_loss: 4.4589e-04\n",
      "Epoch 103/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 0.0011 - val_loss: 4.7364e-04\n",
      "Epoch 104/300\n",
      "1333/1333 [==============================] - 0s 64us/step - loss: 0.0011 - val_loss: 4.0877e-04\n",
      "Epoch 105/300\n",
      "1333/1333 [==============================] - 0s 67us/step - loss: 0.0010 - val_loss: 5.1187e-04\n",
      "Epoch 106/300\n",
      "1333/1333 [==============================] - 0s 69us/step - loss: 0.0011 - val_loss: 4.6214e-04\n",
      "Epoch 107/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 0.0012 - val_loss: 4.1642e-04\n",
      "Epoch 108/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 0.0010 - val_loss: 4.3720e-04\n",
      "Epoch 109/300\n",
      "1333/1333 [==============================] - 0s 76us/step - loss: 8.9204e-04 - val_loss: 5.2379e-04\n",
      "Epoch 110/300\n",
      "1333/1333 [==============================] - 0s 70us/step - loss: 0.0010 - val_loss: 4.0654e-04\n",
      "Epoch 111/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 0.0010 - val_loss: 4.1056e-04\n",
      "Epoch 112/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 0.0010 - val_loss: 7.0690e-04\n",
      "Epoch 113/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 9.3260e-04 - val_loss: 4.0487e-04\n",
      "Epoch 114/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 0.0011 - val_loss: 6.3293e-04\n",
      "Epoch 115/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 9.8028e-04 - val_loss: 5.0780e-04\n",
      "Epoch 116/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 0.0010 - val_loss: 5.8741e-04\n",
      "Epoch 117/300\n",
      "1333/1333 [==============================] - 0s 64us/step - loss: 0.0010 - val_loss: 4.6360e-04\n",
      "Epoch 118/300\n",
      "1333/1333 [==============================] - 0s 67us/step - loss: 9.8405e-04 - val_loss: 5.2778e-04\n",
      "Epoch 119/300\n",
      "1333/1333 [==============================] - 0s 76us/step - loss: 9.2613e-04 - val_loss: 4.4092e-04\n",
      "Epoch 120/300\n",
      "1333/1333 [==============================] - 0s 66us/step - loss: 0.0010 - val_loss: 4.2883e-04\n",
      "Epoch 121/300\n",
      "1333/1333 [==============================] - 0s 67us/step - loss: 0.0012 - val_loss: 8.4547e-04\n",
      "Epoch 122/300\n",
      "1333/1333 [==============================] - 0s 65us/step - loss: 0.0010 - val_loss: 4.5537e-04\n",
      "Epoch 123/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 0.0010 - val_loss: 6.2989e-04\n",
      "Epoch 124/300\n",
      "1333/1333 [==============================] - 0s 65us/step - loss: 0.0011 - val_loss: 5.8638e-04\n",
      "Epoch 125/300\n",
      "1333/1333 [==============================] - 0s 73us/step - loss: 0.0011 - val_loss: 4.4910e-04\n",
      "Epoch 126/300\n",
      "1333/1333 [==============================] - 0s 66us/step - loss: 9.5949e-04 - val_loss: 4.1763e-04\n",
      "Epoch 127/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 0.0011 - val_loss: 4.0159e-04\n",
      "Epoch 128/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 0.0010 - val_loss: 5.1460e-04\n",
      "Epoch 129/300\n",
      "1333/1333 [==============================] - 0s 65us/step - loss: 9.8358e-04 - val_loss: 3.9773e-04\n",
      "Epoch 130/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 8.8804e-04 - val_loss: 6.9300e-04\n",
      "Epoch 131/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 0.0011 - val_loss: 4.6657e-04\n",
      "Epoch 132/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 8.4728e-04 - val_loss: 3.9504e-04\n",
      "Epoch 133/300\n",
      "1333/1333 [==============================] - 0s 64us/step - loss: 9.1529e-04 - val_loss: 4.1219e-04\n",
      "Epoch 134/300\n",
      "1333/1333 [==============================] - 0s 74us/step - loss: 9.7088e-04 - val_loss: 4.0065e-04\n",
      "Epoch 135/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 0.0011 - val_loss: 6.5756e-04\n",
      "Epoch 136/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 0.0010 - val_loss: 3.9821e-04\n",
      "Epoch 137/300\n",
      "1333/1333 [==============================] - 0s 70us/step - loss: 0.0011 - val_loss: 5.8231e-04\n",
      "Epoch 138/300\n",
      "1333/1333 [==============================] - 0s 72us/step - loss: 0.0011 - val_loss: 4.1948e-04\n",
      "Epoch 139/300\n",
      "1333/1333 [==============================] - 0s 65us/step - loss: 9.0528e-04 - val_loss: 4.9456e-04\n",
      "Epoch 140/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 9.1530e-04 - val_loss: 4.0484e-04\n",
      "Epoch 141/300\n",
      "1333/1333 [==============================] - 0s 71us/step - loss: 8.8828e-04 - val_loss: 5.6097e-04\n",
      "Epoch 142/300\n",
      "1333/1333 [==============================] - 0s 68us/step - loss: 8.9580e-04 - val_loss: 3.8721e-04\n",
      "Epoch 143/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 9.3508e-04 - val_loss: 4.9766e-04\n",
      "Epoch 144/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 9.6564e-04 - val_loss: 4.1523e-04\n",
      "Epoch 145/300\n",
      "1333/1333 [==============================] - 0s 74us/step - loss: 9.0242e-04 - val_loss: 4.3880e-04\n",
      "Epoch 146/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 9.4505e-04 - val_loss: 3.8480e-04\n",
      "Epoch 147/300\n",
      "1333/1333 [==============================] - 0s 64us/step - loss: 9.6144e-04 - val_loss: 4.4724e-04\n",
      "Epoch 148/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 0.0010 - val_loss: 4.2480e-04\n",
      "Epoch 149/300\n",
      "1333/1333 [==============================] - 0s 68us/step - loss: 9.7970e-04 - val_loss: 3.8287e-04\n",
      "Epoch 150/300\n",
      "1333/1333 [==============================] - 0s 69us/step - loss: 8.3583e-04 - val_loss: 5.6151e-04\n",
      "Epoch 151/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 9.1938e-04 - val_loss: 5.1050e-04\n",
      "Epoch 152/300\n",
      "1333/1333 [==============================] - 0s 69us/step - loss: 9.7860e-04 - val_loss: 7.3986e-04\n",
      "Epoch 153/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 9.7459e-04 - val_loss: 3.9600e-04\n",
      "Epoch 154/300\n",
      "1333/1333 [==============================] - 0s 73us/step - loss: 8.5206e-04 - val_loss: 4.4776e-04\n",
      "Epoch 155/300\n",
      "1333/1333 [==============================] - 0s 71us/step - loss: 9.1994e-04 - val_loss: 6.4932e-04\n",
      "Epoch 156/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 9.9068e-04 - val_loss: 4.0521e-04\n",
      "Epoch 157/300\n",
      "1333/1333 [==============================] - 0s 78us/step - loss: 8.5741e-04 - val_loss: 4.6471e-04\n",
      "Epoch 158/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 9.0931e-04 - val_loss: 4.2727e-04\n",
      "Epoch 159/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 8.0072e-04 - val_loss: 4.2293e-04\n",
      "Epoch 160/300\n",
      "1333/1333 [==============================] - 0s 57us/step - loss: 8.6653e-04 - val_loss: 4.3801e-04\n",
      "Epoch 161/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 8.9435e-04 - val_loss: 3.7988e-04\n",
      "Epoch 162/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 9.1245e-04 - val_loss: 3.8676e-04\n",
      "Epoch 163/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 8.8956e-04 - val_loss: 7.5462e-04\n",
      "Epoch 164/300\n",
      "1333/1333 [==============================] - 0s 70us/step - loss: 9.7243e-04 - val_loss: 3.9306e-04\n",
      "Epoch 165/300\n",
      "1333/1333 [==============================] - 0s 68us/step - loss: 8.3522e-04 - val_loss: 4.1145e-04\n",
      "Epoch 166/300\n",
      "1333/1333 [==============================] - 0s 57us/step - loss: 9.2521e-04 - val_loss: 4.8960e-04\n",
      "Epoch 167/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 8.0302e-04 - val_loss: 3.8551e-04\n",
      "Epoch 168/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 8.7781e-04 - val_loss: 3.9395e-04\n",
      "Epoch 169/300\n",
      "1333/1333 [==============================] - 0s 67us/step - loss: 9.1644e-04 - val_loss: 4.2205e-04\n",
      "Epoch 170/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 8.7417e-04 - val_loss: 3.9291e-04\n",
      "Epoch 171/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 9.5020e-04 - val_loss: 4.0557e-04\n",
      "Epoch 172/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 9.0713e-04 - val_loss: 4.5710e-04\n",
      "Epoch 173/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 8.1316e-04 - val_loss: 4.4859e-04\n",
      "Epoch 174/300\n",
      "1333/1333 [==============================] - 0s 75us/step - loss: 9.1261e-04 - val_loss: 4.6851e-04\n",
      "Epoch 175/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 8.8752e-04 - val_loss: 3.9125e-04\n",
      "Epoch 176/300\n",
      "1333/1333 [==============================] - 0s 57us/step - loss: 9.3682e-04 - val_loss: 4.4990e-04\n",
      "Epoch 177/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 8.8708e-04 - val_loss: 4.0952e-04\n",
      "Epoch 178/300\n",
      "1333/1333 [==============================] - 0s 55us/step - loss: 7.9244e-04 - val_loss: 3.6921e-04\n",
      "Epoch 179/300\n",
      "1333/1333 [==============================] - 0s 70us/step - loss: 8.4203e-04 - val_loss: 3.9694e-04\n",
      "Epoch 180/300\n",
      "1333/1333 [==============================] - 0s 57us/step - loss: 9.6269e-04 - val_loss: 5.1541e-04\n",
      "Epoch 181/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 8.2972e-04 - val_loss: 4.1019e-04\n",
      "Epoch 182/300\n",
      "1333/1333 [==============================] - 0s 65us/step - loss: 7.9631e-04 - val_loss: 3.7029e-04\n",
      "Epoch 183/300\n",
      "1333/1333 [==============================] - 0s 86us/step - loss: 9.4871e-04 - val_loss: 5.4940e-04\n",
      "Epoch 184/300\n",
      "1333/1333 [==============================] - 0s 92us/step - loss: 8.9247e-04 - val_loss: 3.8185e-04\n",
      "Epoch 185/300\n",
      "1333/1333 [==============================] - 0s 110us/step - loss: 7.9126e-04 - val_loss: 3.8000e-04\n",
      "Epoch 186/300\n",
      "1333/1333 [==============================] - 0s 89us/step - loss: 9.5890e-04 - val_loss: 3.9133e-04\n",
      "Epoch 187/300\n",
      "1333/1333 [==============================] - 0s 86us/step - loss: 8.8364e-04 - val_loss: 3.9036e-04\n",
      "Epoch 188/300\n",
      "1333/1333 [==============================] - 0s 72us/step - loss: 9.3415e-04 - val_loss: 4.4319e-04\n",
      "Epoch 189/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 9.2474e-04 - val_loss: 4.5559e-04\n",
      "Epoch 190/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 8.2578e-04 - val_loss: 4.0619e-04\n",
      "Epoch 191/300\n",
      "1333/1333 [==============================] - 0s 74us/step - loss: 8.4555e-04 - val_loss: 3.6209e-04\n",
      "Epoch 192/300\n",
      "1333/1333 [==============================] - 0s 69us/step - loss: 8.8902e-04 - val_loss: 4.0786e-04\n",
      "Epoch 193/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 9.0533e-04 - val_loss: 4.1518e-04\n",
      "Epoch 194/300\n",
      "1333/1333 [==============================] - 0s 73us/step - loss: 8.7502e-04 - val_loss: 5.5699e-04\n",
      "Epoch 195/300\n",
      "1333/1333 [==============================] - 0s 79us/step - loss: 8.8779e-04 - val_loss: 5.7676e-04\n",
      "Epoch 196/300\n",
      "1333/1333 [==============================] - 0s 72us/step - loss: 8.4563e-04 - val_loss: 3.7740e-04\n",
      "Epoch 197/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 7.6901e-04 - val_loss: 4.0627e-04\n",
      "Epoch 198/300\n",
      "1333/1333 [==============================] - 0s 90us/step - loss: 8.3370e-04 - val_loss: 3.5860e-04\n",
      "Epoch 199/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 8.4940e-04 - val_loss: 3.5905e-04\n",
      "Epoch 200/300\n",
      "1333/1333 [==============================] - 0s 56us/step - loss: 8.4292e-04 - val_loss: 5.3514e-04\n",
      "Epoch 201/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 7.7217e-04 - val_loss: 7.0911e-04\n",
      "Epoch 202/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 9.1824e-04 - val_loss: 6.0687e-04\n",
      "Epoch 203/300\n",
      "1333/1333 [==============================] - 0s 57us/step - loss: 8.9292e-04 - val_loss: 5.2383e-04\n",
      "Epoch 204/300\n",
      "1333/1333 [==============================] - 0s 69us/step - loss: 9.6654e-04 - val_loss: 3.5532e-04\n",
      "Epoch 205/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 9.7386e-04 - val_loss: 6.2458e-04\n",
      "Epoch 206/300\n",
      "1333/1333 [==============================] - 0s 64us/step - loss: 8.1204e-04 - val_loss: 4.1608e-04\n",
      "Epoch 207/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 9.0116e-04 - val_loss: 5.1502e-04\n",
      "Epoch 208/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 8.0851e-04 - val_loss: 4.1569e-04\n",
      "Epoch 209/300\n",
      "1333/1333 [==============================] - 0s 56us/step - loss: 7.9623e-04 - val_loss: 3.5585e-04\n",
      "Epoch 210/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 7.9783e-04 - val_loss: 3.7829e-04\n",
      "Epoch 211/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 7.5300e-04 - val_loss: 4.9643e-04\n",
      "Epoch 212/300\n",
      "1333/1333 [==============================] - 0s 56us/step - loss: 8.1552e-04 - val_loss: 3.9201e-04\n",
      "Epoch 213/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 8.2808e-04 - val_loss: 5.1353e-04\n",
      "Epoch 214/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 9.4519e-04 - val_loss: 3.7591e-04\n",
      "Epoch 215/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 9.1436e-04 - val_loss: 3.8909e-04\n",
      "Epoch 216/300\n",
      "1333/1333 [==============================] - 0s 82us/step - loss: 8.0027e-04 - val_loss: 3.8099e-04\n",
      "Epoch 217/300\n",
      "1333/1333 [==============================] - 0s 66us/step - loss: 8.0160e-04 - val_loss: 4.3461e-04\n",
      "Epoch 218/300\n",
      "1333/1333 [==============================] - 0s 67us/step - loss: 8.2298e-04 - val_loss: 3.9114e-04\n",
      "Epoch 219/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 7.3180e-04 - val_loss: 3.5793e-04\n",
      "Epoch 220/300\n",
      "1333/1333 [==============================] - 0s 67us/step - loss: 8.2708e-04 - val_loss: 4.3574e-04\n",
      "Epoch 221/300\n",
      "1333/1333 [==============================] - 0s 76us/step - loss: 7.7559e-04 - val_loss: 3.6037e-04\n",
      "Epoch 222/300\n",
      "1333/1333 [==============================] - 0s 66us/step - loss: 7.6803e-04 - val_loss: 3.5394e-04\n",
      "Epoch 223/300\n",
      "1333/1333 [==============================] - 0s 69us/step - loss: 8.0371e-04 - val_loss: 4.7028e-04\n",
      "Epoch 224/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 7.9618e-04 - val_loss: 3.8230e-04\n",
      "Epoch 225/300\n",
      "1333/1333 [==============================] - 0s 65us/step - loss: 7.7664e-04 - val_loss: 4.0222e-04\n",
      "Epoch 226/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 7.5875e-04 - val_loss: 4.5115e-04\n",
      "Epoch 227/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 8.2381e-04 - val_loss: 4.2017e-04\n",
      "Epoch 228/300\n",
      "1333/1333 [==============================] - 0s 72us/step - loss: 7.9896e-04 - val_loss: 5.9267e-04\n",
      "Epoch 229/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 8.0229e-04 - val_loss: 4.1014e-04\n",
      "Epoch 230/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 8.4118e-04 - val_loss: 3.5220e-04\n",
      "Epoch 231/300\n",
      "1333/1333 [==============================] - 0s 56us/step - loss: 7.8537e-04 - val_loss: 3.9483e-04\n",
      "Epoch 232/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 8.0742e-04 - val_loss: 3.7831e-04\n",
      "Epoch 233/300\n",
      "1333/1333 [==============================] - 0s 75us/step - loss: 8.6449e-04 - val_loss: 3.5551e-04\n",
      "Epoch 234/300\n",
      "1333/1333 [==============================] - 0s 74us/step - loss: 8.8667e-04 - val_loss: 3.8098e-04\n",
      "Epoch 235/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 8.3760e-04 - val_loss: 3.7290e-04\n",
      "Epoch 236/300\n",
      "1333/1333 [==============================] - 0s 56us/step - loss: 7.9495e-04 - val_loss: 4.7543e-04\n",
      "Epoch 237/300\n",
      "1333/1333 [==============================] - 0s 64us/step - loss: 8.3382e-04 - val_loss: 3.5469e-04\n",
      "Epoch 238/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 7.4782e-04 - val_loss: 5.8165e-04\n",
      "Epoch 239/300\n",
      "1333/1333 [==============================] - 0s 57us/step - loss: 7.7586e-04 - val_loss: 4.4909e-04\n",
      "Epoch 240/300\n",
      "1333/1333 [==============================] - 0s 57us/step - loss: 7.6518e-04 - val_loss: 3.4994e-04\n",
      "Epoch 241/300\n",
      "1333/1333 [==============================] - 0s 70us/step - loss: 8.2812e-04 - val_loss: 3.4536e-04\n",
      "Epoch 242/300\n",
      "1333/1333 [==============================] - 0s 66us/step - loss: 7.4196e-04 - val_loss: 3.9728e-04\n",
      "Epoch 243/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 8.1279e-04 - val_loss: 3.5166e-04\n",
      "Epoch 244/300\n",
      "1333/1333 [==============================] - 0s 68us/step - loss: 8.2803e-04 - val_loss: 3.4694e-04\n",
      "Epoch 245/300\n",
      "1333/1333 [==============================] - 0s 67us/step - loss: 8.1034e-04 - val_loss: 3.4852e-04\n",
      "Epoch 246/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 8.2279e-04 - val_loss: 3.5625e-04\n",
      "Epoch 247/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 7.2523e-04 - val_loss: 4.3768e-04\n",
      "Epoch 248/300\n",
      "1333/1333 [==============================] - 0s 64us/step - loss: 8.0954e-04 - val_loss: 3.8437e-04\n",
      "Epoch 249/300\n",
      "1333/1333 [==============================] - 0s 69us/step - loss: 7.3296e-04 - val_loss: 6.3056e-04\n",
      "Epoch 250/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 7.9310e-04 - val_loss: 6.7196e-04\n",
      "Epoch 251/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 8.1937e-04 - val_loss: 4.7638e-04\n",
      "Epoch 252/300\n",
      "1333/1333 [==============================] - 0s 56us/step - loss: 7.9417e-04 - val_loss: 3.4278e-04\n",
      "Epoch 253/300\n",
      "1333/1333 [==============================] - 0s 67us/step - loss: 7.6452e-04 - val_loss: 3.5061e-04\n",
      "Epoch 254/300\n",
      "1333/1333 [==============================] - 0s 66us/step - loss: 7.4169e-04 - val_loss: 3.4356e-04\n",
      "Epoch 255/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 8.8775e-04 - val_loss: 5.3047e-04\n",
      "Epoch 256/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 7.8604e-04 - val_loss: 6.3243e-04\n",
      "Epoch 257/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 7.8565e-04 - val_loss: 3.6274e-04\n",
      "Epoch 258/300\n",
      "1333/1333 [==============================] - 0s 72us/step - loss: 7.8426e-04 - val_loss: 3.3706e-04\n",
      "Epoch 259/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 8.1744e-04 - val_loss: 3.3468e-04\n",
      "Epoch 260/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 7.6645e-04 - val_loss: 4.2080e-04\n",
      "Epoch 261/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 8.0023e-04 - val_loss: 4.7715e-04\n",
      "Epoch 262/300\n",
      "1333/1333 [==============================] - 0s 68us/step - loss: 7.7990e-04 - val_loss: 3.3684e-04\n",
      "Epoch 263/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 8.1150e-04 - val_loss: 9.2670e-04\n",
      "Epoch 264/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 8.4236e-04 - val_loss: 3.8571e-04\n",
      "Epoch 265/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 7.8536e-04 - val_loss: 3.9197e-04\n",
      "Epoch 266/300\n",
      "1333/1333 [==============================] - 0s 72us/step - loss: 7.3919e-04 - val_loss: 3.7072e-04\n",
      "Epoch 267/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 7.3490e-04 - val_loss: 3.6853e-04\n",
      "Epoch 268/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 7.4925e-04 - val_loss: 3.5340e-04\n",
      "Epoch 269/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 7.9484e-04 - val_loss: 4.7876e-04\n",
      "Epoch 270/300\n",
      "1333/1333 [==============================] - 0s 66us/step - loss: 7.3165e-04 - val_loss: 4.8681e-04\n",
      "Epoch 271/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 7.6177e-04 - val_loss: 3.5970e-04\n",
      "Epoch 272/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 7.5701e-04 - val_loss: 4.1033e-04\n",
      "Epoch 273/300\n",
      "1333/1333 [==============================] - 0s 56us/step - loss: 7.8879e-04 - val_loss: 3.3683e-04\n",
      "Epoch 274/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 7.1874e-04 - val_loss: 6.3991e-04\n",
      "Epoch 275/300\n",
      "1333/1333 [==============================] - 0s 57us/step - loss: 7.9113e-04 - val_loss: 3.4501e-04\n",
      "Epoch 276/300\n",
      "1333/1333 [==============================] - 0s 57us/step - loss: 8.0701e-04 - val_loss: 4.3260e-04\n",
      "Epoch 277/300\n",
      "1333/1333 [==============================] - 0s 60us/step - loss: 7.9015e-04 - val_loss: 4.7581e-04\n",
      "Epoch 278/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 6.8494e-04 - val_loss: 5.3048e-04\n",
      "Epoch 279/300\n",
      "1333/1333 [==============================] - 0s 74us/step - loss: 7.8603e-04 - val_loss: 3.4033e-04\n",
      "Epoch 280/300\n",
      "1333/1333 [==============================] - 0s 75us/step - loss: 7.9046e-04 - val_loss: 3.3729e-04\n",
      "Epoch 281/300\n",
      "1333/1333 [==============================] - 0s 67us/step - loss: 7.0170e-04 - val_loss: 4.9739e-04\n",
      "Epoch 282/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 8.1017e-04 - val_loss: 5.6736e-04\n",
      "Epoch 283/300\n",
      "1333/1333 [==============================] - 0s 75us/step - loss: 7.6622e-04 - val_loss: 7.0664e-04\n",
      "Epoch 284/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 9.1103e-04 - val_loss: 3.7917e-04\n",
      "Epoch 285/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 8.2066e-04 - val_loss: 3.3082e-04\n",
      "Epoch 286/300\n",
      "1333/1333 [==============================] - 0s 62us/step - loss: 7.2574e-04 - val_loss: 3.5301e-04\n",
      "Epoch 287/300\n",
      "1333/1333 [==============================] - 0s 59us/step - loss: 7.0804e-04 - val_loss: 3.4272e-04\n",
      "Epoch 288/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 7.4951e-04 - val_loss: 5.4221e-04\n",
      "Epoch 289/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 7.8377e-04 - val_loss: 9.9011e-04\n",
      "Epoch 290/300\n",
      "1333/1333 [==============================] - 0s 57us/step - loss: 8.5415e-04 - val_loss: 5.5545e-04\n",
      "Epoch 291/300\n",
      "1333/1333 [==============================] - 0s 70us/step - loss: 7.3293e-04 - val_loss: 3.3860e-04\n",
      "Epoch 292/300\n",
      "1333/1333 [==============================] - 0s 65us/step - loss: 7.8571e-04 - val_loss: 3.9058e-04\n",
      "Epoch 293/300\n",
      "1333/1333 [==============================] - 0s 69us/step - loss: 7.4410e-04 - val_loss: 3.8927e-04\n",
      "Epoch 294/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 8.0742e-04 - val_loss: 3.2613e-04\n",
      "Epoch 295/300\n",
      "1333/1333 [==============================] - 0s 95us/step - loss: 7.9639e-04 - val_loss: 3.8420e-04\n",
      "Epoch 296/300\n",
      "1333/1333 [==============================] - 0s 84us/step - loss: 7.4501e-04 - val_loss: 3.8871e-04\n",
      "Epoch 297/300\n",
      "1333/1333 [==============================] - 0s 69us/step - loss: 7.6340e-04 - val_loss: 4.4797e-04\n",
      "Epoch 298/300\n",
      "1333/1333 [==============================] - 0s 58us/step - loss: 7.5341e-04 - val_loss: 3.2745e-04\n",
      "Epoch 299/300\n",
      "1333/1333 [==============================] - 0s 61us/step - loss: 7.1822e-04 - val_loss: 3.3065e-04\n",
      "Epoch 300/300\n",
      "1333/1333 [==============================] - 0s 63us/step - loss: 7.8002e-04 - val_loss: 3.4573e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(trainX,trainY,batch_size=,epochs = ,validation_split=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación del modelo y valores de pérdida "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "cmkSxYuk0xaV",
    "outputId": "2650b7b1-24ea-470d-b334-395cec1f24c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9b51322588>]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAdHElEQVR4nO3dfYxdd53f8ffnnHvnzsRxHIdMKRvH\n2GGNul6xStCsYcuSSksCDivFqRqEabfNSkgRW6xSoUoEsQrUK6Ql2yJ11RTICqsUNetA6KpT1TTN\nkizbLSR4QkKCQ00c8xB7AzG28+Cnmbn3fvvH+c343Iex79hj7szZz0saz3m893vOufdzf/6dM/co\nIjAzs+rKhl2AmZldWg56M7OKc9CbmVWcg97MrOIc9GZmFVcbdgHdrr766tiwYcOwyzAzW1GeeOKJ\nX0TEeL95yy7oN2zYwNTU1LDLMDNbUST9ZKF57roxM6s4B72ZWcU56M3MKs5Bb2ZWcQ56M7OKc9Cb\nmVWcg97MrOKqE/TTJ+CRT8MhX4NvZlZWnaBvnoG/vgcOf3fYlZiZLSvVCXqlTYnWcOswM1tmqhP0\nWV78bjvozczKqhP0SkHvFr2ZWYfqBL1b9GZmfVUn6N2iNzPrqzpBP9+ibw+3DjOzZaY6Qe+rbszM\n+qpQ0KsIe/fRm5l1qE7QQxH04a4bM7OyigV97q4bM7Mu1Qr6LHfXjZlZl4GCXtJWSfslHZB0V5/5\nH5L0jKSnJP2NpM1p+gZJp9P0pyR9fqk3oLOQ3F03ZmZdaudbQFIO3AvcDBwC9kqajIhnS4vdHxGf\nT8vfCnwW2JrmPR8R1y9t2QvIfDLWzKzbIC36LcCBiDgYETPAbmBbeYGIeLU0ugqIpStxEdxHb2bW\nY5CgvwZ4oTR+KE3rIOnDkp4H7gH+VWnWRklPSvqmpHf2ewJJd0qakjR15MiRRZTfxX30ZmY9luxk\nbETcGxFvAj4G/GGa/CKwPiJuAD4K3C/pij7r3hcRExExMT4+fuFFuEVvZtZjkKA/DFxbGl+Xpi1k\nN3AbQERMR8TRNPwE8Dzw5gsrdQBZ7q9AMDPrMkjQ7wU2SdooaQTYDkyWF5C0qTT6u8Bzafp4OpmL\npOuATcDBpSi8L7fozcx6nPeqm4hoStoBPATkwK6I2CdpJzAVEZPADkk3AbPAceCOtPqNwE5Js0Ab\n+FBEHLsUGwL4qhszsz7OG/QAEbEH2NM17e7S8EcWWO9rwNcupsBFcYvezKyH/zLWzKziqhX0btGb\nmfWoVtD7qhszsx7VCnplbtGbmXWpVtC7j97MrEe1gt43HjEz61GxoPfJWDOzbtUKenfdmJn1qFbQ\n+8YjZmY9qhX0/goEM7Me1Qp699GbmfWoVtC7j97MrEe1gt4tejOzHtUKen8FgplZj2oFvb8Cwcys\nR7WC3n30ZmY9qhX07qM3M+sxUNBL2ippv6QDku7qM/9Dkp6R9JSkv5G0uTTv42m9/ZLes5TF93CL\n3sysx3mDPt3c+17gFmAz8IFykCf3R8RbIuJ64B7gs2ndzRQ3E/91YCvwn+ZuFn5JuEVvZtZjkBb9\nFuBARByMiBlgN7CtvEBEvFoaXQVEGt4G7I6I6Yj4EXAgPd6l4atuzMx6DHJz8GuAF0rjh4C3dS8k\n6cPAR4ER4HdK6z7Wte41fda9E7gTYP369YPU3Z+vujEz67FkJ2Mj4t6IeBPwMeAPF7nufRExERET\n4+PjF16E++jNzHoMEvSHgWtL4+vStIXsBm67wHUvjlv0ZmY9Bgn6vcAmSRsljVCcXJ0sLyBpU2n0\nd4Hn0vAksF1SQ9JGYBPwnYsvewH+mmIzsx7n7aOPiKakHcBDQA7sioh9knYCUxExCeyQdBMwCxwH\n7kjr7pP0FeBZoAl8OOISNrl9MtbMrMcgJ2OJiD3Anq5pd5eGP3KOdT8NfPpCC1wUX15pZtajWn8Z\n6xuPmJn1qFbQu0VvZtajWkHvyyvNzHpUK+jdojcz61GtoM/S1+j4yhszs3nVCvq570tzq97MbF61\ngj5Lm+N+ejOzedUKerfozcx6VCvo5/voHfRmZnOqFfRu0ZuZ9ahW0PuqGzOzHtUKeqXNcYvezGxe\ntYLeffRmZj2qFfTuozcz61GxoJ/runEfvZnZnGoFvbtuzMx6VCvo57tu3KI3M5szUNBL2ippv6QD\nku7qM/+jkp6V9LSkb0h6Y2leS9JT6Weye90l5Ra9mVmP895KUFIO3AvcDBwC9kqajIhnS4s9CUxE\nxClJfwDcA7w/zTsdEdcvcd0LFOvLK83Mug3Sot8CHIiIgxExA+wGtpUXiIhHI+JUGn0MWLe0ZQ7I\nLXozsx6DBP01wAul8UNp2kI+CHy9ND4qaUrSY5Ju67eCpDvTMlNHjhwZoKQF+PJKM7Me5+26WQxJ\nvwdMAP+oNPmNEXFY0nXAI5KeiYjny+tFxH3AfQATExNxwQW4RW9m1mOQFv1h4NrS+Lo0rYOkm4BP\nALdGxPTc9Ig4nH4fBP4KuOEi6j03X3VjZtZjkKDfC2yStFHSCLAd6Lh6RtINwBcoQv6l0vS1khpp\n+GrgHUD5JO7S8o1HzMx6nLfrJiKaknYADwE5sCsi9knaCUxFxCTwJ8DlwFclAfw0Im4Ffg34gqQ2\nxYfKH3ddrbO03EdvZtZjoD76iNgD7Omadndp+KYF1vsW8JaLKXBR3EdvZtajon8Z66A3M5tTraB3\ni97MrEe1gt5X3ZiZ9ahW0PuqGzOzHtUKevfRm5n1qFjQu0VvZtatWkGfuY/ezKxbtYLeXTdmZj2q\nFfTzl1e6RW9mNqdaQe8bj5iZ9ahW0PsPpszMelQr6N1Hb2bWo1pB7xa9mVmPagW9W/RmZj2qFfS+\n6sbMrEe1gt5X3ZiZ9ahW0LuP3sysx0BBL2mrpP2SDki6q8/8j0p6VtLTkr4h6Y2leXdIei793LGU\nxfcW6j56M7Nu5w16STlwL3ALsBn4gKTNXYs9CUxExG8ADwL3pHWvAj4JvA3YAnxS0tqlK7+LW/Rm\nZj0GadFvAQ5ExMGImAF2A9vKC0TEoxFxKo0+BqxLw+8BHo6IYxFxHHgY2Lo0pffhFr2ZWY9Bgv4a\n4IXS+KE0bSEfBL6+mHUl3SlpStLUkSNHBihpAb7qxsysx5KejJX0e8AE8CeLWS8i7ouIiYiYGB8f\nv4gCfNWNmVm3QYL+MHBtaXxdmtZB0k3AJ4BbI2J6MesuGakIe/fRm5nNGyTo9wKbJG2UNAJsBybL\nC0i6AfgCRci/VJr1EPBuSWvTSdh3p2mXjjLfeMTMrKR2vgUioilpB0VA58CuiNgnaScwFRGTFF01\nlwNflQTw04i4NSKOSfojig8LgJ0RceySbMkc5e66MTMrOW/QA0TEHmBP17S7S8M3nWPdXcCuCy1w\n0bLcXTdmZiXV+stYSC16d92Ymc2pXtBnPhlrZlZWvaB3H72ZWYfqBb376M3MOlQv6N2iNzPrUL2g\nz3J/BYKZWUn1gt4tejOzDtULel91Y2bWoXpB7xa9mVmH6gW9r7oxM+tQvaB3i97MrEP1gt5X3ZiZ\ndahe0Ctzi97MrKR6Qe8+ejOzDtULerfozcw6VDDo/TXFZmZl1Qt6d92YmXUYKOglbZW0X9IBSXf1\nmX+jpO9Kakq6vWteS9JT6Weye92lMtNs88RPjjPTllv0ZmYl5w16STlwL3ALsBn4gKTNXYv9FPh9\n4P4+D3E6Iq5PP7deZL0LevXMLP/kc9/i6KmmW/RmZiWD3DN2C3AgIg4CSNoNbAOenVsgIn6c5g2t\nKT1azwFohU/GmpmVDdJ1cw3wQmn8UJo2qFFJU5Iek3TboqpbzJPUik1p4S81MzMrG6RFf7HeGBGH\nJV0HPCLpmYh4vryApDuBOwHWr19/QU9SyzNqmWiG3KI3MysZpEV/GLi2NL4uTRtIRBxOvw8CfwXc\n0GeZ+yJiIiImxsfHB33oHqP1nCaZvwLBzKxkkKDfC2yStFHSCLAdGOjqGUlrJTXS8NXAOyj17S+1\nRi1zi97MrMt5gz4imsAO4CHgB8BXImKfpJ2SbgWQ9JuSDgHvA74gaV9a/deAKUnfAx4F/jgiLlnQ\nj9Zzmm330ZuZlQ3URx8Re4A9XdPuLg3vpejS6V7vW8BbLrLGgTXqGU3cojczK6vUX8aO1nJm3aI3\nM+tQraCvu4/ezKxbxYI+Zzbkq27MzEoqFfSNWsZs2y16M7OySgX9aD0vvtTMffRmZvMqF/Sz7qM3\nM+tQsaDPmG3jryk2MyupVNA3annRR++TsWZm8yoV9PN99O66MTObV6mgb9QyZtoifDLWzGxepYJ+\ntJ7TxjceMTMrq1jQZ77xiJlZl4oFfU6LDLlFb2Y2r2JBn9GOtEm+8sbMDKha0NeKFj3gfnozs6RS\nQd+oZ8XJWHA/vZlZUqmgd4vezKxXpYK+US8FvVv0ZmbAgEEvaauk/ZIOSLqrz/wbJX1XUlPS7V3z\n7pD0XPq5Y6kK72e03HXjFr2ZGTBA0EvKgXuBW4DNwAckbe5a7KfA7wP3d617FfBJ4G3AFuCTktZe\nfNn9jXa06H3VjZkZDNai3wIciIiDETED7Aa2lReIiB9HxNNAd7q+B3g4Io5FxHHgYWDrEtTdV6OW\nuY/ezKzLIEF/DfBCafxQmjaIgdaVdKekKUlTR44cGfChe81/BQK4j97MLFkWJ2Mj4r6ImIiIifHx\n8Qt+nI6uG7fozcyAwYL+MHBtaXxdmjaIi1l30UZrGW1UjLhFb2YGDBb0e4FNkjZKGgG2A5MDPv5D\nwLslrU0nYd+dpl0StTwDzbXofTLWzAwGCPqIaAI7KAL6B8BXImKfpJ2SbgWQ9JuSDgHvA74gaV9a\n9xjwRxQfFnuBnWnaJZPntVS4g97MDKA2yEIRsQfY0zXt7tLwXopumX7r7gJ2XUSNi5JlNQjcdWNm\nliyLk7FLKa/Ntegd9GZmUMGgz/K8GHCL3swMqGDQ13K36M3MyioX9PNdN27Rm5kBFQz6Ws1X3ZiZ\nlVUu6HP30ZuZdahc0Ndq9WLAffRmZkAlg9599GZmZdUNerfozcyASgZ96rpxi97MDKhg0Nd91Y2Z\nWYfKBf1c10271RxyJWZmy0Plgr5eL7puZpsOejMzqGDQj9SL6+hnZ2eHXImZ2fJQuaCv19yiNzMr\nq1zQj9SLPnoHvZlZoYJBX7Tom0133ZiZwYBBL2mrpP2SDki6q8/8hqQH0vzHJW1I0zdIOi3pqfTz\n+aUtv9f8ydhZX0dvZgYD3EpQUg7cC9wMHAL2SpqMiGdLi30QOB4RvyppO/AZ4P1p3vMRcf0S170g\nt+jNzDoN0qLfAhyIiIMRMQPsBrZ1LbMN+FIafhB4lyQtXZmD8+WVZmadBgn6a4AXSuOH0rS+y0RE\nE3gFeF2at1HSk5K+Kemd/Z5A0p2SpiRNHTlyZFEb0K0xMteid9CbmcGlPxn7IrA+Im4APgrcL+mK\n7oUi4r6ImIiIifHx8Yt6wkZq0bf8l7FmZsBgQX8YuLY0vi5N67uMpBqwBjgaEdMRcRQgIp4Angfe\nfLFFn8tci77lFr2ZGTBY0O8FNknaKGkE2A5Mdi0zCdyRhm8HHomIkDSeTuYi6TpgE3BwaUrvb/5k\nrFv0ZmbAAFfdRERT0g7gISAHdkXEPkk7gamImAS+CHxZ0gHgGMWHAcCNwE5Js0Ab+FBEHLsUGzJn\ndGQEgOasg97MDAYIeoCI2APs6Zp2d2n4DPC+Put9DfjaRda4KJeNjQIwPXPml/m0ZmbLVuX+Mlb1\nMVpktM+8NuxSzMyWhcoFPRKnNUZMnxh2JWZmy0L1gh6YzsbQjIPezAwqGvSz+WVksyeHXYaZ2bJQ\nyaBv1VZRb54adhlmZstCJYO+Xb+cRpxipukbhJuZVTLoaaxiFdMcPzUz7ErMzIaukkGfNVazitMc\nPeGgNzOrZNDXxlazSmc4dtJBb2Y20F/GrjQjY6sZ4zRHT04PuxQzs6GrZIu+sWoNo5rl6Ku+8sbM\nrJJBP3r5GgC++u3/x6kZf7mZmf3dVsmgzxqXA/Dy8eN88f/8aMjVmJkNVyWDnpEi6N++rsH/ePpv\nh1yMmdlwVTPoG6sBeNd1Y/zw5yc48JK/ydLM/u6qZtCPrALgHetH+a1sHw9888khF2RmNjwVDfqi\n62btz77Nn498mvXf+w98/ZkXh1yUmdlwDBT0krZK2i/pgKS7+sxvSHogzX9c0obSvI+n6fslvWfp\nSj+H1HXDt/4UgPeOPMnHHnyKbz9/lFY7fiklmJktF+f9g6l0c+97gZuBQ8BeSZMR8WxpsQ8CxyPi\nVyVtBz4DvF/SZor7x/468CvAX0p6c0S0lnpDOqSuG9pNePNWXvfD/8UN+UH+2Z81WTNa41euupxa\nJvJM1PLs7PB5xut5Rp6J6WaLUzMtGrWcsXrOSC2jpiY12kQ+Sp6WlyBXMZxJZKIYTuO5iuE8g0wC\n4OTpM6weGyXPM9oRtAPq6bmb7TbTzTa1LKNRy2jUM3IJtaaJrA5Kn9vq3B3qmqDu+a0ZRBB5o7RM\naaFoo/Ys1Bqd67VbZK3TtOuXL/jY3cX0zu9aot1Cee/LsrxMNnOCqDcgq5fmdz1wBFKc3ScL6NkX\n56s3AsUs5CMLP3dpPbWmibzRZ4kl2Fd0HacFl+me3zkhCGZbAQRXjNbnH3N+GxZ4Ps1PW+Cx02BE\n0GoXr+XiNR1EQJQWk4r3QPk9A3SsQxQLi2I5UbRUJZCyYp6g2QpOTjep5aKWFe/ZSO+lIKDVglNH\naa/6e2RZ6T2aiSh2A0FnjRFnG4hnp3Xuwzn1LGOklhXvqVf/llZ9FTGyptiGrn02t7/mni/PxRWj\nZ1/XS2WQv4zdAhyIiINFgdoNbAPKQb8N+FQafhD4jypeEduA3RExDfwo3Tx8C/DtpSl/AXMt+vX/\nEG77HPy7Tfzn9ifQaLGj28cyZlXszDyatCnGW+TzDzF3TMrt/0j/hEoHKII6TVbrNACnY4QTjBF9\n39rnVqPJVTrBbOScZJQRmrQRTfKexwugVbzMGdcrnIwGJxgr3jQUr9azw8VYMaT54UjDV/EqNVq8\nzOUdy9VpkhHktBhjhmOspk1GABnBFZyioVmOxBW0yEtrFjLaNJilSc5seqmJdnqDdv6eoUaLjKt5\nhde4jDFmgGCGOjPUaM8vCa/Xy0xHnQBqtGiSM0ONJjlNasySs5YTNJjhOKs5w9lQntsDnb975wG0\nUbG9IWpqsZpT5LQZ0ww/i7W0yMgIsrRX5pZvp/1ao8Xf13GOxBWciUbxwVN6nvLzd9cUaU+1yRil\n+CqPM4wwxjQ5bU7RoBX5/GNq/mj2DrPA9Lnh2XR8ruQEx1mdXlcQIaRglBlmqdFghiY1ZqgRMfdh\n0P9/yOX9mBGs5hRNcl6Ly9J+a5MpyNMey2gzzQirOEOdJqdpcDpG5l83ZXP7e1QzXMkJctocZzUn\nY5SGZmkww9Vpn70UVyKCLNXZjJwrdJKrdIKfx5VMR73vceneR2e36ey8rOs9dnbZoA3ktFmladoh\nfs5aTkXj7PaqeN3ktGmRMR112mT8bOxNvOPj/7PvPr0YgwT9NcALpfFDwNsWWiYimpJeAV6Xpj/W\nte413U8g6U7gToD169cPWvvCRlYVAb/hnXDZVfDP/wL9+P8W87KcrDlNozUNCLIaRJuR5jR0/0cj\n+r2I+0zLajC2FrIao6eP0zjzWnpJkFoFqXUw16IIaFN8SMw9RTsClPPa6tczM32abPYE0/kIImg3\nZ2lHFP8ryATRptVqEa0WtNscuuz11GZeJWueJpRebgI6XqapgPT77IsZfjFyJe28QX362NllgKZq\nhIqljo6soX76CKINETQlXqpfQWtkNY0Th0p7Zq6pIppknMlHULtZ/I8gTQ+y+d+RmjZZWuaF0aup\nz7zMq7VVhDKy1gxqz6BooWgDwfOXXUOteRIiaGd11G4W68csas+StZscqV9BqzbKyPTLZO3ps4ey\n1Pxsd9XbsUzaR0Txxmwp5xf14oOuVRtj7NTP0j5Kb/eOdYr1ZoEfXvYGRs+8hNqzHbE+/9xzrd75\no0GxTyJQtFG0eS0fAUTeOsOreYNQjbx1GrVb88cn5vet5h+3HFd9p6V11Z6lHjMcGlkDp18uqpl/\nHQQns1HymOWVbJSs3SRrz3S8D8qNkM4GieYa4RwbWU0ebUaar87vz5bEDDmhIrrz9hlOZmM0sxHy\n5mnqrdNktNIjlp5DGS3glazBS/U1tMlpzB6n1jzFdNagnTfIRsaIdpvG9NHig1d5seXR4lg+wk9W\nbWDNiech2rSjHOFzx+XsPoMsNe7o2G+g+f0/v73FoaP4hnTx2mXraDRfY83pF8jbs7QkmioabpF+\nK9rk7Wky2oxfeR2XwrL4rpuIuA+4D2BiYmJpOtGv/6dnhzfeWPz8EpQPu5nZcjDIydjDwLWl8XVp\nWt9lJNWANcDRAdc1M7NLaJCg3wtskrRR0gjFydXJrmUmgTvS8O3AI1GcvZgEtqercjYCm4DvLE3p\nZmY2iPN23aQ+9x3AQ0AO7IqIfZJ2AlMRMQl8EfhyOtl6jOLDgLTcVyhO3DaBD1/yK27MzKyDou8J\nx+GZmJiIqampYZdhZraiSHoiIib6zavmX8aamdk8B72ZWcU56M3MKs5Bb2ZWccvuZKykI8BPLuIh\nrgZ+sUTlDFtVtqUq2wHeluXK2wJvjIjxfjOWXdBfLElTC515Xmmqsi1V2Q7wtixX3pZzc9eNmVnF\nOejNzCquikF/37ALWEJV2ZaqbAd4W5Yrb8s5VK6P3szMOlWxRW9mZiUOejOziqtM0J/vBubLnaQf\nS3pG0lOSptK0qyQ9LOm59HvtsOvsR9IuSS9J+n5pWt/aVfjTdJyelvTW4VXea4Ft+ZSkw+nYPCXp\nvaV5H0/bsl/Se4ZTdX+SrpX0qKRnJe2T9JE0fUUdm3Nsx4o7LpJGJX1H0vfStvzbNH2jpMdTzQ+k\nr4QnfcX7A2n645I2XNATF7ezW9k/FF+f/DxwHTACfA/YPOy6FrkNPwau7pp2D3BXGr4L+Myw61yg\n9huBtwLfP1/twHuBr1PciOvtwOPDrn+AbfkU8G/6LLs5vdYawMb0GsyHvQ2l+t4AvDUNrwZ+mGpe\nUcfmHNux4o5L2reXp+E68Hja118Btqfpnwf+IA3/S+DzaXg78MCFPG9VWvTzNzCPiBlg7gbmK902\n4Etp+EvAbUOsZUER8dcU9yEoW6j2bcB/icJjwJWS3vDLqfT8FtiWhWwDdkfEdET8CDhA8VpcFiLi\nxYj4bhp+DfgBxT2bV9SxOcd2LGTZHpe0b0+k0Xr6CeB3gAfT9O5jMnesHgTeJWnRdyutStD3u4H5\nuV4Iy1EA/1vSE+lm6QCvj4gX0/DPgNcPp7QLslDtK/VY7UjdGbtKXWgrZlvSf/lvoGhBrthj07Ud\nsAKPi6Rc0lPAS8DDFP/jeDkimmmRcr3z25LmvwK8brHPWZWgr4Lfjoi3ArcAH5bUcTfzKP7vtiKv\nhV3JtSefA94EXA+8CPz74ZazOJIuB74G/OuIeLU8byUdmz7bsSKPS0S0IuJ6intobwH+waV+zqoE\n/Yq/CXlEHE6/XwL+guIF8PO5/zqn3y8Nr8JFW6j2FXesIuLn6c3ZBv6Ms90Ay35bJNUpwvG/RsR/\nS5NX3LHptx0r+bgARMTLwKPAb1F0k83d2rVc7/y2pPlrgKOLfa6qBP0gNzBftiStkrR6bhh4N/B9\nOm+6fgfw34dT4QVZqPZJ4F+kKzzeDrxS6kZYlrr6qf8xxbGBYlu2pysjNgKbgO/8sutbSOrL/SLw\ng4j4bGnWijo2C23HSjwuksYlXZmGx4CbKc45PArcnhbrPiZzx+p24JH0v7DFGfZZ6KX6obhi4IcU\n/V2fGHY9i6z9OoqrBL4H7Jurn6Iv7hvAc8BfAlcNu9YF6v9ziv86z1L0L35wodoprjq4Nx2nZ4CJ\nYdc/wLZ8OdX6dHrjvaG0/CfStuwHbhl2/V3b8tsU3TJPA0+ln/eutGNzju1YcccF+A3gyVTz94G7\n0/TrKD6MDgBfBRpp+mgaP5DmX3chz+uvQDAzq7iqdN2YmdkCHPRmZhXnoDczqzgHvZlZxTnozcwq\nzkFvZlZxDnozs4r7/wYSj1amJFzsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Podéis cambiarlo para dibujar en seaborn que es algo más mono\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representar (bien) las predicciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "ERoCo6Uv1TEQ",
    "outputId": "3917e496-94db-49df-998c-dadecaa235ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9b40341cf8>]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xt = model.predict(testX)\n",
    "# Añadid el dibujo y dibujad las predicciones bien hechas \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TJBQ0Y0z2yXg",
    "outputId": "8543459a-6d2e-401f-dff1-b8af610c6913"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9697014861091255"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# cuidado con lo que metéis ahi ;)\n",
    "\n",
    "r2_score(,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluación en cadena\n",
    "\n",
    "Evaluad la salida del modelo y añadidlo a la entrada del siguiente paso para predecir 6 dias consecutivos usando el mismo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oxr6TB3z5hTQ"
   },
   "outputs": [],
   "source": [
    "a =np.reshape(testX[0],(1,1,testX[0].shape[1]))\n",
    "lista = []\n",
    "for i in range(0,6):\n",
    "  b = # prediccion\n",
    "  a = # coger solo los ultimos tres\n",
    "  a = # añadir b en cuarto lugar\n",
    "  a = # reescalar \n",
    "  lista.append(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valor de R2 para esos 6 dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LuBFMbHJ6DvF",
    "outputId": "38321e84-4477-4ae4-9db5-0f8de1d92eb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rellenar\n",
    "r2_score( ,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representación de las predicciones vs los valores esperados esos 6 dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dibujicos!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear dataset 2\n",
    "\n",
    "Crear conjunto de datos para predecir en cada paso los ``future_steps`` dias siguientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZxkcs-__23K"
   },
   "outputs": [],
   "source": [
    "def create_dataset_2(dataset, look_back,future_steps):\n",
    "    # TODO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nuevo conjunto de datos para predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los dos parámetros\n",
    "look_back = \n",
    "future_steps = \n",
    "trainX, trainY = create_dataset_2(train, look_back,future_steps)\n",
    "testX, testY = create_dataset_2(test, look_back,future_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corregir las dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFuIUzYBAoM1"
   },
   "outputs": [],
   "source": [
    "# Rellenad las dimensiones\n",
    "trainX = np.reshape(trainX, )\n",
    "testX = np.reshape(testX, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nuevo modelo con salida mayor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "gPf1NoDSBtQ7",
    "outputId": "1424e672-52f2-49c5-ed0d-65c610819c83"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=True, input_shape=(None, 7), units=50)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compilation time :  0.024686336517333984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=3)`\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Rellenar\n",
    "\n",
    "model.compile(loss=, optimizer=)\n",
    "print ('compilation time : ', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "tqomNHYAB2Ca",
    "outputId": "b62bd703-aed1-4c86-a80e-318c7495bf88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1328 samples, validate on 70 samples\n",
      "Epoch 1/300\n",
      "1328/1328 [==============================] - 1s 997us/step - loss: 0.1628 - val_loss: 0.4391\n",
      "Epoch 2/300\n",
      "1328/1328 [==============================] - 0s 69us/step - loss: 0.1247 - val_loss: 0.3030\n",
      "Epoch 3/300\n",
      "1328/1328 [==============================] - 0s 87us/step - loss: 0.0704 - val_loss: 0.1126\n",
      "Epoch 4/300\n",
      "1328/1328 [==============================] - 0s 73us/step - loss: 0.0217 - val_loss: 0.0013\n",
      "Epoch 5/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 0.0154 - val_loss: 0.0011\n",
      "Epoch 6/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0094 - val_loss: 0.0079\n",
      "Epoch 7/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0075 - val_loss: 0.0025\n",
      "Epoch 8/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 0.0052 - val_loss: 7.4471e-04\n",
      "Epoch 9/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 0.0041 - val_loss: 9.1445e-04\n",
      "Epoch 10/300\n",
      "1328/1328 [==============================] - 0s 64us/step - loss: 0.0036 - val_loss: 7.5468e-04\n",
      "Epoch 11/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 0.0031 - val_loss: 8.0018e-04\n",
      "Epoch 12/300\n",
      "1328/1328 [==============================] - 0s 68us/step - loss: 0.0029 - val_loss: 7.8127e-04\n",
      "Epoch 13/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0030 - val_loss: 9.5590e-04\n",
      "Epoch 14/300\n",
      "1328/1328 [==============================] - 0s 64us/step - loss: 0.0031 - val_loss: 9.9324e-04\n",
      "Epoch 15/300\n",
      "1328/1328 [==============================] - 0s 67us/step - loss: 0.0028 - val_loss: 8.5804e-04\n",
      "Epoch 16/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0029 - val_loss: 8.2043e-04\n",
      "Epoch 17/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0029 - val_loss: 8.3663e-04\n",
      "Epoch 18/300\n",
      "1328/1328 [==============================] - 0s 67us/step - loss: 0.0030 - val_loss: 7.6712e-04\n",
      "Epoch 19/300\n",
      "1328/1328 [==============================] - 0s 74us/step - loss: 0.0027 - val_loss: 8.3454e-04\n",
      "Epoch 20/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 0.0029 - val_loss: 9.0182e-04\n",
      "Epoch 21/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0027 - val_loss: 7.5258e-04\n",
      "Epoch 22/300\n",
      "1328/1328 [==============================] - 0s 85us/step - loss: 0.0025 - val_loss: 7.2898e-04\n",
      "Epoch 23/300\n",
      "1328/1328 [==============================] - 0s 73us/step - loss: 0.0023 - val_loss: 7.3530e-04\n",
      "Epoch 24/300\n",
      "1328/1328 [==============================] - 0s 82us/step - loss: 0.0026 - val_loss: 8.0639e-04\n",
      "Epoch 25/300\n",
      "1328/1328 [==============================] - 0s 64us/step - loss: 0.0023 - val_loss: 7.3268e-04\n",
      "Epoch 26/300\n",
      "1328/1328 [==============================] - 0s 64us/step - loss: 0.0024 - val_loss: 8.5292e-04\n",
      "Epoch 27/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 0.0024 - val_loss: 7.7865e-04\n",
      "Epoch 28/300\n",
      "1328/1328 [==============================] - 0s 65us/step - loss: 0.0021 - val_loss: 8.4460e-04\n",
      "Epoch 29/300\n",
      "1328/1328 [==============================] - 0s 69us/step - loss: 0.0021 - val_loss: 7.6163e-04\n",
      "Epoch 30/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0022 - val_loss: 9.4132e-04\n",
      "Epoch 31/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0021 - val_loss: 7.9078e-04\n",
      "Epoch 32/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 0.0022 - val_loss: 7.8452e-04\n",
      "Epoch 33/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 0.0021 - val_loss: 7.8661e-04\n",
      "Epoch 34/300\n",
      "1328/1328 [==============================] - 0s 66us/step - loss: 0.0020 - val_loss: 9.1658e-04\n",
      "Epoch 35/300\n",
      "1328/1328 [==============================] - 0s 67us/step - loss: 0.0021 - val_loss: 8.3607e-04\n",
      "Epoch 36/300\n",
      "1328/1328 [==============================] - 0s 67us/step - loss: 0.0020 - val_loss: 7.0388e-04\n",
      "Epoch 37/300\n",
      "1328/1328 [==============================] - 0s 57us/step - loss: 0.0020 - val_loss: 9.6257e-04\n",
      "Epoch 38/300\n",
      "1328/1328 [==============================] - 0s 77us/step - loss: 0.0020 - val_loss: 7.0181e-04\n",
      "Epoch 39/300\n",
      "1328/1328 [==============================] - 0s 64us/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 40/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0019 - val_loss: 7.2991e-04\n",
      "Epoch 41/300\n",
      "1328/1328 [==============================] - 0s 57us/step - loss: 0.0019 - val_loss: 9.2250e-04\n",
      "Epoch 42/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0018 - val_loss: 8.3972e-04\n",
      "Epoch 43/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0018 - val_loss: 7.4752e-04\n",
      "Epoch 44/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0018 - val_loss: 8.1337e-04\n",
      "Epoch 45/300\n",
      "1328/1328 [==============================] - 0s 76us/step - loss: 0.0017 - val_loss: 7.5451e-04\n",
      "Epoch 46/300\n",
      "1328/1328 [==============================] - 0s 86us/step - loss: 0.0020 - val_loss: 8.7175e-04\n",
      "Epoch 47/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 0.0018 - val_loss: 7.0718e-04\n",
      "Epoch 48/300\n",
      "1328/1328 [==============================] - 0s 83us/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 49/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 0.0019 - val_loss: 7.8065e-04\n",
      "Epoch 50/300\n",
      "1328/1328 [==============================] - 0s 73us/step - loss: 0.0019 - val_loss: 8.6280e-04\n",
      "Epoch 51/300\n",
      "1328/1328 [==============================] - 0s 69us/step - loss: 0.0017 - val_loss: 7.2075e-04\n",
      "Epoch 52/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 53/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0017 - val_loss: 6.8432e-04\n",
      "Epoch 54/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0016 - val_loss: 9.9698e-04\n",
      "Epoch 55/300\n",
      "1328/1328 [==============================] - 0s 57us/step - loss: 0.0018 - val_loss: 6.9354e-04\n",
      "Epoch 56/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 57/300\n",
      "1328/1328 [==============================] - 0s 80us/step - loss: 0.0018 - val_loss: 7.3017e-04\n",
      "Epoch 58/300\n",
      "1328/1328 [==============================] - 0s 80us/step - loss: 0.0016 - val_loss: 8.8730e-04\n",
      "Epoch 59/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 0.0017 - val_loss: 7.0938e-04\n",
      "Epoch 60/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 0.0018 - val_loss: 8.6025e-04\n",
      "Epoch 61/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 0.0017 - val_loss: 8.1663e-04\n",
      "Epoch 62/300\n",
      "1328/1328 [==============================] - 0s 77us/step - loss: 0.0017 - val_loss: 8.4941e-04\n",
      "Epoch 63/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0016 - val_loss: 8.7465e-04\n",
      "Epoch 64/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 0.0017 - val_loss: 7.2890e-04\n",
      "Epoch 65/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0014 - val_loss: 7.9841e-04\n",
      "Epoch 66/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0016 - val_loss: 9.9183e-04\n",
      "Epoch 67/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 0.0015 - val_loss: 6.9045e-04\n",
      "Epoch 68/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0017 - val_loss: 9.8447e-04\n",
      "Epoch 69/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0017 - val_loss: 7.9364e-04\n",
      "Epoch 70/300\n",
      "1328/1328 [==============================] - 0s 71us/step - loss: 0.0018 - val_loss: 7.7905e-04\n",
      "Epoch 71/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0015 - val_loss: 6.9954e-04\n",
      "Epoch 72/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0016 - val_loss: 9.8940e-04\n",
      "Epoch 73/300\n",
      "1328/1328 [==============================] - 0s 64us/step - loss: 0.0016 - val_loss: 7.0172e-04\n",
      "Epoch 74/300\n",
      "1328/1328 [==============================] - 0s 69us/step - loss: 0.0014 - val_loss: 8.6391e-04\n",
      "Epoch 75/300\n",
      "1328/1328 [==============================] - 0s 72us/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 76/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0015 - val_loss: 7.3072e-04\n",
      "Epoch 77/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0016 - val_loss: 8.4619e-04\n",
      "Epoch 78/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0017 - val_loss: 8.6295e-04\n",
      "Epoch 79/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 80/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0015 - val_loss: 7.0302e-04\n",
      "Epoch 81/300\n",
      "1328/1328 [==============================] - 0s 57us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 82/300\n",
      "1328/1328 [==============================] - 0s 66us/step - loss: 0.0015 - val_loss: 7.3127e-04\n",
      "Epoch 83/300\n",
      "1328/1328 [==============================] - 0s 75us/step - loss: 0.0014 - val_loss: 7.5130e-04\n",
      "Epoch 84/300\n",
      "1328/1328 [==============================] - 0s 56us/step - loss: 0.0015 - val_loss: 7.9260e-04\n",
      "Epoch 85/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 86/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0015 - val_loss: 6.6268e-04\n",
      "Epoch 87/300\n",
      "1328/1328 [==============================] - 0s 69us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 88/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0014 - val_loss: 6.6644e-04\n",
      "Epoch 89/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 0.0015 - val_loss: 7.5330e-04\n",
      "Epoch 90/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 0.0015 - val_loss: 8.9367e-04\n",
      "Epoch 91/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0015 - val_loss: 6.9692e-04\n",
      "Epoch 92/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0015 - val_loss: 9.7266e-04\n",
      "Epoch 93/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 0.0014 - val_loss: 7.8801e-04\n",
      "Epoch 94/300\n",
      "1328/1328 [==============================] - 0s 68us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 95/300\n",
      "1328/1328 [==============================] - 0s 64us/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 96/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0014 - val_loss: 6.5870e-04\n",
      "Epoch 97/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 0.0014 - val_loss: 7.2747e-04\n",
      "Epoch 98/300\n",
      "1328/1328 [==============================] - 0s 65us/step - loss: 0.0014 - val_loss: 8.8004e-04\n",
      "Epoch 99/300\n",
      "1328/1328 [==============================] - 0s 67us/step - loss: 0.0013 - val_loss: 9.9398e-04\n",
      "Epoch 100/300\n",
      "1328/1328 [==============================] - 0s 72us/step - loss: 0.0014 - val_loss: 6.5528e-04\n",
      "Epoch 101/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 102/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0013 - val_loss: 9.2733e-04\n",
      "Epoch 103/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 0.0014 - val_loss: 6.7282e-04\n",
      "Epoch 104/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0014 - val_loss: 7.3175e-04\n",
      "Epoch 105/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 106/300\n",
      "1328/1328 [==============================] - 0s 72us/step - loss: 0.0015 - val_loss: 9.0716e-04\n",
      "Epoch 107/300\n",
      "1328/1328 [==============================] - 0s 67us/step - loss: 0.0014 - val_loss: 6.9410e-04\n",
      "Epoch 108/300\n",
      "1328/1328 [==============================] - 0s 73us/step - loss: 0.0014 - val_loss: 6.3413e-04\n",
      "Epoch 109/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0014 - val_loss: 6.4016e-04\n",
      "Epoch 110/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0013 - val_loss: 8.9225e-04\n",
      "Epoch 111/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0012 - val_loss: 9.7199e-04\n",
      "Epoch 112/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0012 - val_loss: 7.0234e-04\n",
      "Epoch 113/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0013 - val_loss: 8.1526e-04\n",
      "Epoch 114/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 0.0012 - val_loss: 6.3128e-04\n",
      "Epoch 115/300\n",
      "1328/1328 [==============================] - 0s 57us/step - loss: 0.0014 - val_loss: 8.1287e-04\n",
      "Epoch 116/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 117/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0013 - val_loss: 8.0980e-04\n",
      "Epoch 118/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 0.0013 - val_loss: 7.0088e-04\n",
      "Epoch 119/300\n",
      "1328/1328 [==============================] - 0s 66us/step - loss: 0.0013 - val_loss: 6.3246e-04\n",
      "Epoch 120/300\n",
      "1328/1328 [==============================] - 0s 77us/step - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 121/300\n",
      "1328/1328 [==============================] - 0s 71us/step - loss: 0.0012 - val_loss: 8.8564e-04\n",
      "Epoch 122/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 0.0013 - val_loss: 6.0495e-04\n",
      "Epoch 123/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 0.0013 - val_loss: 9.4119e-04\n",
      "Epoch 124/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 125/300\n",
      "1328/1328 [==============================] - 0s 68us/step - loss: 0.0013 - val_loss: 8.7286e-04\n",
      "Epoch 126/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 127/300\n",
      "1328/1328 [==============================] - 0s 57us/step - loss: 0.0012 - val_loss: 9.4225e-04\n",
      "Epoch 128/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0013 - val_loss: 6.1542e-04\n",
      "Epoch 129/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0012 - val_loss: 6.1968e-04\n",
      "Epoch 130/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 131/300\n",
      "1328/1328 [==============================] - 0s 64us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 132/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0012 - val_loss: 6.0415e-04\n",
      "Epoch 133/300\n",
      "1328/1328 [==============================] - 0s 65us/step - loss: 0.0011 - val_loss: 7.6698e-04\n",
      "Epoch 134/300\n",
      "1328/1328 [==============================] - 0s 68us/step - loss: 0.0012 - val_loss: 7.5586e-04\n",
      "Epoch 135/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 0.0012 - val_loss: 6.5240e-04\n",
      "Epoch 136/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0013 - val_loss: 7.1194e-04\n",
      "Epoch 137/300\n",
      "1328/1328 [==============================] - 0s 66us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 138/300\n",
      "1328/1328 [==============================] - 0s 75us/step - loss: 0.0011 - val_loss: 6.1485e-04\n",
      "Epoch 139/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 0.0012 - val_loss: 6.5934e-04\n",
      "Epoch 140/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 141/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 0.0011 - val_loss: 6.7895e-04\n",
      "Epoch 142/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0012 - val_loss: 8.2090e-04\n",
      "Epoch 143/300\n",
      "1328/1328 [==============================] - 0s 76us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 144/300\n",
      "1328/1328 [==============================] - 0s 71us/step - loss: 0.0011 - val_loss: 6.2295e-04\n",
      "Epoch 145/300\n",
      "1328/1328 [==============================] - 0s 67us/step - loss: 0.0011 - val_loss: 8.2195e-04\n",
      "Epoch 146/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0011 - val_loss: 7.9471e-04\n",
      "Epoch 147/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0012 - val_loss: 6.9293e-04\n",
      "Epoch 148/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 0.0011 - val_loss: 5.7758e-04\n",
      "Epoch 149/300\n",
      "1328/1328 [==============================] - 0s 65us/step - loss: 0.0012 - val_loss: 8.1100e-04\n",
      "Epoch 150/300\n",
      "1328/1328 [==============================] - 0s 101us/step - loss: 0.0011 - val_loss: 6.0240e-04\n",
      "Epoch 151/300\n",
      "1328/1328 [==============================] - 0s 98us/step - loss: 0.0012 - val_loss: 5.9994e-04\n",
      "Epoch 152/300\n",
      "1328/1328 [==============================] - 0s 98us/step - loss: 0.0011 - val_loss: 8.5236e-04\n",
      "Epoch 153/300\n",
      "1328/1328 [==============================] - 0s 108us/step - loss: 0.0011 - val_loss: 9.5238e-04\n",
      "Epoch 154/300\n",
      "1328/1328 [==============================] - 0s 89us/step - loss: 0.0011 - val_loss: 5.9206e-04\n",
      "Epoch 155/300\n",
      "1328/1328 [==============================] - 0s 76us/step - loss: 0.0012 - val_loss: 8.0033e-04\n",
      "Epoch 156/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 0.0012 - val_loss: 6.9044e-04\n",
      "Epoch 157/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0011 - val_loss: 5.8787e-04\n",
      "Epoch 158/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0012 - val_loss: 8.1794e-04\n",
      "Epoch 159/300\n",
      "1328/1328 [==============================] - 0s 69us/step - loss: 0.0011 - val_loss: 6.6961e-04\n",
      "Epoch 160/300\n",
      "1328/1328 [==============================] - 0s 67us/step - loss: 0.0011 - val_loss: 7.0404e-04\n",
      "Epoch 161/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 162/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 0.0011 - val_loss: 8.6029e-04\n",
      "Epoch 163/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 0.0011 - val_loss: 7.7174e-04\n",
      "Epoch 164/300\n",
      "1328/1328 [==============================] - 0s 68us/step - loss: 0.0011 - val_loss: 6.9561e-04\n",
      "Epoch 165/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0011 - val_loss: 7.9016e-04\n",
      "Epoch 166/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 167/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0012 - val_loss: 8.4399e-04\n",
      "Epoch 168/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0011 - val_loss: 5.8369e-04\n",
      "Epoch 169/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0010 - val_loss: 6.1500e-04\n",
      "Epoch 170/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 171/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 0.0011 - val_loss: 9.0394e-04\n",
      "Epoch 172/300\n",
      "1328/1328 [==============================] - 0s 76us/step - loss: 0.0012 - val_loss: 6.8436e-04\n",
      "Epoch 173/300\n",
      "1328/1328 [==============================] - 0s 74us/step - loss: 0.0011 - val_loss: 5.6305e-04\n",
      "Epoch 174/300\n",
      "1328/1328 [==============================] - 0s 78us/step - loss: 0.0011 - val_loss: 5.7488e-04\n",
      "Epoch 175/300\n",
      "1328/1328 [==============================] - 0s 69us/step - loss: 0.0011 - val_loss: 7.4015e-04\n",
      "Epoch 176/300\n",
      "1328/1328 [==============================] - 0s 81us/step - loss: 0.0012 - val_loss: 9.8080e-04\n",
      "Epoch 177/300\n",
      "1328/1328 [==============================] - 0s 66us/step - loss: 0.0011 - val_loss: 7.6861e-04\n",
      "Epoch 178/300\n",
      "1328/1328 [==============================] - 0s 76us/step - loss: 0.0010 - val_loss: 7.5001e-04\n",
      "Epoch 179/300\n",
      "1328/1328 [==============================] - 0s 69us/step - loss: 0.0011 - val_loss: 6.5724e-04\n",
      "Epoch 180/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0011 - val_loss: 8.9889e-04\n",
      "Epoch 181/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 182/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 9.9096e-04 - val_loss: 9.4784e-04\n",
      "Epoch 183/300\n",
      "1328/1328 [==============================] - 0s 77us/step - loss: 0.0011 - val_loss: 6.0683e-04\n",
      "Epoch 184/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 0.0011 - val_loss: 5.7952e-04\n",
      "Epoch 185/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0011 - val_loss: 5.5583e-04\n",
      "Epoch 186/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 0.0011 - val_loss: 5.5440e-04\n",
      "Epoch 187/300\n",
      "1328/1328 [==============================] - 0s 66us/step - loss: 9.4016e-04 - val_loss: 6.6503e-04\n",
      "Epoch 188/300\n",
      "1328/1328 [==============================] - 0s 66us/step - loss: 0.0011 - val_loss: 6.9307e-04\n",
      "Epoch 189/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0010 - val_loss: 7.4386e-04\n",
      "Epoch 190/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0011 - val_loss: 7.8666e-04\n",
      "Epoch 191/300\n",
      "1328/1328 [==============================] - 0s 80us/step - loss: 9.7102e-04 - val_loss: 6.9641e-04\n",
      "Epoch 192/300\n",
      "1328/1328 [==============================] - 0s 64us/step - loss: 9.5419e-04 - val_loss: 6.8915e-04\n",
      "Epoch 193/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 0.0010 - val_loss: 9.7863e-04\n",
      "Epoch 194/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 0.0010 - val_loss: 7.8235e-04\n",
      "Epoch 195/300\n",
      "1328/1328 [==============================] - 0s 67us/step - loss: 0.0010 - val_loss: 6.4120e-04\n",
      "Epoch 196/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 0.0011 - val_loss: 5.9918e-04\n",
      "Epoch 197/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0010 - val_loss: 6.1554e-04\n",
      "Epoch 198/300\n",
      "1328/1328 [==============================] - 0s 70us/step - loss: 0.0011 - val_loss: 5.7735e-04\n",
      "Epoch 199/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0011 - val_loss: 7.1484e-04\n",
      "Epoch 200/300\n",
      "1328/1328 [==============================] - 0s 72us/step - loss: 0.0010 - val_loss: 6.7839e-04\n",
      "Epoch 201/300\n",
      "1328/1328 [==============================] - 0s 67us/step - loss: 8.6568e-04 - val_loss: 6.2132e-04\n",
      "Epoch 202/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 9.5169e-04 - val_loss: 7.0554e-04\n",
      "Epoch 203/300\n",
      "1328/1328 [==============================] - 0s 69us/step - loss: 9.7872e-04 - val_loss: 7.9077e-04\n",
      "Epoch 204/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0010 - val_loss: 9.0794e-04\n",
      "Epoch 205/300\n",
      "1328/1328 [==============================] - 0s 66us/step - loss: 0.0011 - val_loss: 8.6914e-04\n",
      "Epoch 206/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0010 - val_loss: 5.8992e-04\n",
      "Epoch 207/300\n",
      "1328/1328 [==============================] - 0s 71us/step - loss: 0.0010 - val_loss: 5.9468e-04\n",
      "Epoch 208/300\n",
      "1328/1328 [==============================] - 0s 66us/step - loss: 0.0010 - val_loss: 5.7384e-04\n",
      "Epoch 209/300\n",
      "1328/1328 [==============================] - 0s 64us/step - loss: 9.8725e-04 - val_loss: 7.2346e-04\n",
      "Epoch 210/300\n",
      "1328/1328 [==============================] - 0s 66us/step - loss: 9.5808e-04 - val_loss: 6.3715e-04\n",
      "Epoch 211/300\n",
      "1328/1328 [==============================] - 0s 71us/step - loss: 9.5811e-04 - val_loss: 5.4451e-04\n",
      "Epoch 212/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 9.9538e-04 - val_loss: 5.7662e-04\n",
      "Epoch 213/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 9.9615e-04 - val_loss: 5.7021e-04\n",
      "Epoch 214/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 0.0010 - val_loss: 7.8807e-04\n",
      "Epoch 215/300\n",
      "1328/1328 [==============================] - 0s 69us/step - loss: 9.0635e-04 - val_loss: 6.0482e-04\n",
      "Epoch 216/300\n",
      "1328/1328 [==============================] - 0s 69us/step - loss: 0.0010 - val_loss: 5.7078e-04\n",
      "Epoch 217/300\n",
      "1328/1328 [==============================] - 0s 73us/step - loss: 9.6359e-04 - val_loss: 6.9171e-04\n",
      "Epoch 218/300\n",
      "1328/1328 [==============================] - 0s 80us/step - loss: 9.8793e-04 - val_loss: 7.8363e-04\n",
      "Epoch 219/300\n",
      "1328/1328 [==============================] - 0s 74us/step - loss: 0.0010 - val_loss: 8.2010e-04\n",
      "Epoch 220/300\n",
      "1328/1328 [==============================] - 0s 70us/step - loss: 0.0010 - val_loss: 6.3810e-04\n",
      "Epoch 221/300\n",
      "1328/1328 [==============================] - 0s 82us/step - loss: 0.0010 - val_loss: 9.2854e-04\n",
      "Epoch 222/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 9.2157e-04 - val_loss: 6.7569e-04\n",
      "Epoch 223/300\n",
      "1328/1328 [==============================] - 0s 64us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 224/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 225/300\n",
      "1328/1328 [==============================] - 0s 64us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 226/300\n",
      "1328/1328 [==============================] - 0s 69us/step - loss: 0.0010 - val_loss: 8.8791e-04\n",
      "Epoch 227/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 9.7578e-04 - val_loss: 6.1801e-04\n",
      "Epoch 228/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 9.4193e-04 - val_loss: 7.8765e-04\n",
      "Epoch 229/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 0.0010 - val_loss: 6.9869e-04\n",
      "Epoch 230/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 9.8769e-04 - val_loss: 5.4281e-04\n",
      "Epoch 231/300\n",
      "1328/1328 [==============================] - 0s 77us/step - loss: 9.1118e-04 - val_loss: 5.4589e-04\n",
      "Epoch 232/300\n",
      "1328/1328 [==============================] - 0s 74us/step - loss: 0.0011 - val_loss: 5.5124e-04\n",
      "Epoch 233/300\n",
      "1328/1328 [==============================] - 0s 81us/step - loss: 0.0011 - val_loss: 6.3727e-04\n",
      "Epoch 234/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 9.1849e-04 - val_loss: 5.4200e-04\n",
      "Epoch 235/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 9.6295e-04 - val_loss: 5.5086e-04\n",
      "Epoch 236/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 9.9741e-04 - val_loss: 7.0541e-04\n",
      "Epoch 237/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 9.3890e-04 - val_loss: 0.0014\n",
      "Epoch 238/300\n",
      "1328/1328 [==============================] - 0s 64us/step - loss: 0.0011 - val_loss: 5.9580e-04\n",
      "Epoch 239/300\n",
      "1328/1328 [==============================] - 0s 65us/step - loss: 0.0010 - val_loss: 5.4183e-04\n",
      "Epoch 240/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 9.3511e-04 - val_loss: 6.9968e-04\n",
      "Epoch 241/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 9.2180e-04 - val_loss: 7.4284e-04\n",
      "Epoch 242/300\n",
      "1328/1328 [==============================] - 0s 64us/step - loss: 9.5076e-04 - val_loss: 5.7320e-04\n",
      "Epoch 243/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 0.0010 - val_loss: 5.7174e-04\n",
      "Epoch 244/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 0.0011 - val_loss: 5.3970e-04\n",
      "Epoch 245/300\n",
      "1328/1328 [==============================] - 0s 74us/step - loss: 9.5053e-04 - val_loss: 6.5908e-04\n",
      "Epoch 246/300\n",
      "1328/1328 [==============================] - 0s 73us/step - loss: 8.7572e-04 - val_loss: 7.7853e-04\n",
      "Epoch 247/300\n",
      "1328/1328 [==============================] - 0s 60us/step - loss: 9.3032e-04 - val_loss: 6.4106e-04\n",
      "Epoch 248/300\n",
      "1328/1328 [==============================] - 0s 56us/step - loss: 9.8701e-04 - val_loss: 6.2029e-04\n",
      "Epoch 249/300\n",
      "1328/1328 [==============================] - 0s 65us/step - loss: 8.8095e-04 - val_loss: 5.6898e-04\n",
      "Epoch 250/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 9.3834e-04 - val_loss: 5.5408e-04\n",
      "Epoch 251/300\n",
      "1328/1328 [==============================] - 0s 69us/step - loss: 8.9751e-04 - val_loss: 6.7509e-04\n",
      "Epoch 252/300\n",
      "1328/1328 [==============================] - 0s 65us/step - loss: 9.7114e-04 - val_loss: 5.8169e-04\n",
      "Epoch 253/300\n",
      "1328/1328 [==============================] - 0s 65us/step - loss: 9.1517e-04 - val_loss: 5.3997e-04\n",
      "Epoch 254/300\n",
      "1328/1328 [==============================] - 0s 65us/step - loss: 9.1777e-04 - val_loss: 7.5539e-04\n",
      "Epoch 255/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 8.8275e-04 - val_loss: 9.5305e-04\n",
      "Epoch 256/300\n",
      "1328/1328 [==============================] - 0s 64us/step - loss: 9.1711e-04 - val_loss: 8.0432e-04\n",
      "Epoch 257/300\n",
      "1328/1328 [==============================] - 0s 68us/step - loss: 8.9460e-04 - val_loss: 7.1320e-04\n",
      "Epoch 258/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 0.0010 - val_loss: 7.9625e-04\n",
      "Epoch 259/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 9.1443e-04 - val_loss: 8.8765e-04\n",
      "Epoch 260/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 9.9098e-04 - val_loss: 6.7420e-04\n",
      "Epoch 261/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 262/300\n",
      "1328/1328 [==============================] - 0s 66us/step - loss: 8.9022e-04 - val_loss: 5.8672e-04\n",
      "Epoch 263/300\n",
      "1328/1328 [==============================] - 0s 75us/step - loss: 9.4913e-04 - val_loss: 5.6055e-04\n",
      "Epoch 264/300\n",
      "1328/1328 [==============================] - 0s 70us/step - loss: 0.0010 - val_loss: 9.4404e-04\n",
      "Epoch 265/300\n",
      "1328/1328 [==============================] - 0s 65us/step - loss: 8.8556e-04 - val_loss: 6.0914e-04\n",
      "Epoch 266/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 0.0010 - val_loss: 5.8211e-04\n",
      "Epoch 267/300\n",
      "1328/1328 [==============================] - 0s 75us/step - loss: 9.4932e-04 - val_loss: 5.4271e-04\n",
      "Epoch 268/300\n",
      "1328/1328 [==============================] - 0s 68us/step - loss: 0.0010 - val_loss: 6.5866e-04\n",
      "Epoch 269/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 9.0236e-04 - val_loss: 6.2630e-04\n",
      "Epoch 270/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 0.0010 - val_loss: 8.1105e-04\n",
      "Epoch 271/300\n",
      "1328/1328 [==============================] - 0s 85us/step - loss: 9.7745e-04 - val_loss: 6.4221e-04\n",
      "Epoch 272/300\n",
      "1328/1328 [==============================] - 0s 68us/step - loss: 9.7828e-04 - val_loss: 9.3643e-04\n",
      "Epoch 273/300\n",
      "1328/1328 [==============================] - 0s 87us/step - loss: 0.0011 - val_loss: 7.8915e-04\n",
      "Epoch 274/300\n",
      "1328/1328 [==============================] - 0s 70us/step - loss: 9.5066e-04 - val_loss: 6.0938e-04\n",
      "Epoch 275/300\n",
      "1328/1328 [==============================] - 0s 86us/step - loss: 9.9425e-04 - val_loss: 7.6722e-04\n",
      "Epoch 276/300\n",
      "1328/1328 [==============================] - 0s 95us/step - loss: 9.4972e-04 - val_loss: 8.5294e-04\n",
      "Epoch 277/300\n",
      "1328/1328 [==============================] - 0s 102us/step - loss: 9.4474e-04 - val_loss: 8.4020e-04\n",
      "Epoch 278/300\n",
      "1328/1328 [==============================] - 0s 82us/step - loss: 9.4418e-04 - val_loss: 9.8489e-04\n",
      "Epoch 279/300\n",
      "1328/1328 [==============================] - 0s 88us/step - loss: 9.0080e-04 - val_loss: 0.0010\n",
      "Epoch 280/300\n",
      "1328/1328 [==============================] - 0s 71us/step - loss: 0.0011 - val_loss: 6.5958e-04\n",
      "Epoch 281/300\n",
      "1328/1328 [==============================] - 0s 57us/step - loss: 9.7522e-04 - val_loss: 8.1129e-04\n",
      "Epoch 282/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 9.2434e-04 - val_loss: 6.6390e-04\n",
      "Epoch 283/300\n",
      "1328/1328 [==============================] - 0s 61us/step - loss: 9.1108e-04 - val_loss: 5.5561e-04\n",
      "Epoch 284/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 9.0814e-04 - val_loss: 5.8434e-04\n",
      "Epoch 285/300\n",
      "1328/1328 [==============================] - 0s 84us/step - loss: 8.8728e-04 - val_loss: 5.7340e-04\n",
      "Epoch 286/300\n",
      "1328/1328 [==============================] - 0s 103us/step - loss: 9.7833e-04 - val_loss: 5.5629e-04\n",
      "Epoch 287/300\n",
      "1328/1328 [==============================] - 0s 94us/step - loss: 9.9013e-04 - val_loss: 6.4947e-04\n",
      "Epoch 288/300\n",
      "1328/1328 [==============================] - 0s 91us/step - loss: 0.0010 - val_loss: 7.0453e-04\n",
      "Epoch 289/300\n",
      "1328/1328 [==============================] - 0s 62us/step - loss: 9.0831e-04 - val_loss: 5.9080e-04\n",
      "Epoch 290/300\n",
      "1328/1328 [==============================] - 0s 65us/step - loss: 8.7448e-04 - val_loss: 5.9761e-04\n",
      "Epoch 291/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 9.9389e-04 - val_loss: 6.0130e-04\n",
      "Epoch 292/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 8.4085e-04 - val_loss: 7.2581e-04\n",
      "Epoch 293/300\n",
      "1328/1328 [==============================] - 0s 58us/step - loss: 9.5365e-04 - val_loss: 7.7050e-04\n",
      "Epoch 294/300\n",
      "1328/1328 [==============================] - 0s 59us/step - loss: 9.0981e-04 - val_loss: 0.0014\n",
      "Epoch 295/300\n",
      "1328/1328 [==============================] - 0s 84us/step - loss: 0.0011 - val_loss: 8.1584e-04\n",
      "Epoch 296/300\n",
      "1328/1328 [==============================] - 0s 78us/step - loss: 9.9422e-04 - val_loss: 0.0011\n",
      "Epoch 297/300\n",
      "1328/1328 [==============================] - 0s 77us/step - loss: 8.9680e-04 - val_loss: 0.0010\n",
      "Epoch 298/300\n",
      "1328/1328 [==============================] - 0s 74us/step - loss: 8.7708e-04 - val_loss: 7.5082e-04\n",
      "Epoch 299/300\n",
      "1328/1328 [==============================] - 0s 71us/step - loss: 9.3094e-04 - val_loss: 7.8697e-04\n",
      "Epoch 300/300\n",
      "1328/1328 [==============================] - 0s 63us/step - loss: 8.7619e-04 - val_loss: 5.4012e-04\n"
     ]
    }
   ],
   "source": [
    "# Decidid tamaño de batch, epochs.\n",
    "history = model.fit(trainX,trainY,batch_size=,epochs = ,validation_split=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nuevo valor de R2 para la nueva salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lL6MaGRRB9LF",
    "outputId": "f1aa5721-ab90-4ae7-f593-28df2bbc369d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9545250302065705"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Colocad correctamente las predicciones para calcular el R2\n",
    "from sklearn.metrics import r2_score\n",
    "Xt = model.predict(testX)\n",
    "r2_score(,)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "S4 - Stocks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
