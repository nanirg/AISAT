{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ziY8dujBRHWh"
   },
   "source": [
    "# ***Reto semanal 3: Head Pose***\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tb6ScVgeRVYP"
   },
   "source": [
    " ***1*** Lo primero que debes hacer es importar las librerías necesarias ( para tratar y visualizar datos y estadísticas, manejar ficheros si fuera necesario, tratar redes neuronales...)\n",
    "\n",
    " ejemplo:\n",
    "\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7P6fXXGQ-ZE"
   },
   "outputs": [],
   "source": [
    "#--------------------Librerias--------------------\n",
    "\n",
    "# os dejamos las básicas, añadid las que convengan\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1p6FzVwLSRXp"
   },
   "source": [
    "***2*** Lo siguiente es descargar el dataset de 'http://www-prima.inrialpes.fr/perso/Gourier/Faces/HeadPoseImageDatabase.tar.gz' y estructurar los datos para dejarlos listos para su uso (descompresión del archivo tar.gz, creación de variables, tratamiento de expresiones regulares ...).\n",
    "\n",
    "La información relevante a la construcción del dataset se puede encontrar en http://www-prima.inrialpes.fr/perso/Gourier/Faces/HPDatabase.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iuGnmq8-SpYs"
   },
   "outputs": [],
   "source": [
    "#2.1 Descarga y tratamiento como archivo\n",
    "\n",
    "url = 'http://www-prima.inrialpes.fr/perso/Gourier/Faces/HeadPoseImageDatabase.tar.gz'\n",
    "name = 'HeadPoseImageDatabase.tar.gz'\n",
    "\n",
    "# Librería requests \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2 Descompresión del archivo tar.gz\n",
    "\n",
    "# usando el linux que hay debajo, comando tar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formato del archivo:\n",
    "\n",
    "<p>\n",
    "    Recordad que en el enlace de información se describe cómo está guardada la información, estando en cada archivo de la imágen los ángulos de inclinación y giro (tilt,pan) y las coordenadas de la cara(x,y), altura y anchura (h,w) dentro del archivo\n",
    " </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3 Tratamiento de las expresiones regulares de los títulos de las imagenes para conseguir las caracteristicas\n",
    "\n",
    "# os dejamos esta función para, dado el path de una imágen, transformarla a un tamaño más trabajable\n",
    "\n",
    "def img_df(image_path, shape):\n",
    "    image = Image.open(image_path)\n",
    "    image_resized = image.resize(shape, Image.ANTIALIAS)\n",
    "    img_array = np.asarray(image_resized)\n",
    "    return img_array\n",
    "\n",
    "# Cargar los datos en el dataframe, extraidos de cada archivo (nombre del archivo y contenidos)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "# finalmente deberíais tener algo como lo siguiente\n",
    "\n",
    "df.columns = [\"X\", \"Y\", \"H\", \"W\", \"T\", \"P\", \"Image\"]\n",
    "df.X = df.X.astype(int)\n",
    "df.Y = df.Y.astype(int)\n",
    "df.H = df.H.astype(int)\n",
    "df.W = df.W.astype(int)\n",
    "df = df.reset_index().drop(\"index\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.4 Almacenamiento de datos en archivos (opcional) y separado en X(datos) Y(a predecir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.5 Conjuntos finales de x_train, y_train, x_test y y_test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9f_4xdWAStJU"
   },
   "source": [
    "***3*** Posteriormente se importa la red neuronal mobilenet sin incluir la última capa, crearla nosotros y concatenarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqvp4P2nUHGU"
   },
   "outputs": [],
   "source": [
    "#--------------------Importar y manipular la red para dejarla lista para su entrenamiento--------------------\n",
    "#3.1 Importar la red de keras sin la última capa\n",
    "\n",
    "# Nosotros como hemos dicho os recomendamos Mobilenet\n",
    "from keras.applications import MobileNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2 Crear nuestra última capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.3 Juntar la red y la capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.4 Compilar (elegir optimizador, funcion de perdida(loss) y métrica de error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RwHKpaHlUZTd"
   },
   "source": [
    "***4*** Finalmente se entrena la red que hemos importado y manipulado con el dataset que hemos tratado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2aSH22geUt2e"
   },
   "outputs": [],
   "source": [
    "#--------------------Entrenamiento de nuestra red personalizada--------------------\n",
    "# Un precioso fit() y a esperar. Unas 10 épocas deberian dar un resultado decente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Vwyi4bLUyZn"
   },
   "source": [
    "***5*** Si se desea se puede dibujar un diagrama de correlación entre los valores predichos y los valores que debieran ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JiWhF4VIVIF5"
   },
   "outputs": [],
   "source": [
    "#----Diagrama de correlación y métricas (RMSE,R2, precioso sklearn os lo da pero para cada variable por separado)----\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PlantillaHeadPoseAI6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
